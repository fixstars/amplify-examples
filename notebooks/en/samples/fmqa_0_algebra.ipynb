{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-Box Optimization with Quantum Annealing and Ising Machines\n",
    "\n",
    "This code example introduces a method of black-box optimization, which may be called Factorization Machines and Quantum Annealing (FMQA)$^*$. \n",
    "\n",
    "For this purpose, the sample code considers an algebraic expression as an unknown black-box function and estimates the input values so that its output value is minimized. For examples of FMQA in more realistic model cases and sample code, please see the following links.\n",
    "\n",
    "- [Black-Box Optimization Exploration of Model Superconducting Materials](https://amplify.fixstars.com/en/demo/fmqa_1_supercon)\n",
    "- [Black-Box Optimization of Operating Condition in a Chemical Reactor](https://amplify.fixstars.com/en/demo/fmqa_2_reactor)\n",
    "- [Black-Box Optimization of Airfoil Geometry by Fluid Flow Simulation](https://amplify.fixstars.com/en/demo/fmqa_3_aerofoil)\n",
    "\n",
    "If the optimization target is a known function and quadratic, it can be optimized directly in the form of Quadratic Unconstrained Binary Optimization (QUBO) form. For a description of such the QUBO problem, sample code, and instructions on how to use *Amplify*, please see the following links (excerpts).\n",
    "\n",
    "- [First-Time Users](https://amplify.fixstars.com/en/demo/1-tutorial-basic)\n",
    "- [Combinatorial Optimization](https://amplify.fixstars.com/en/demo/1-tutorial-combinatorial-optimization)\n",
    "- [Traveling Salesman Problem](https://amplify.fixstars.com/en/demo/tsp)\n",
    "\n",
    "This notebook is organized as follows:\n",
    "\n",
    "- 1\\. [Introduction to FMQA](#1)\n",
    "  - 1.1\\. [Black-box optimization](#1_1)\n",
    "  - 1.2\\. [Bayesian optimization](#1_2)\n",
    "  - 1.3\\. [FMQA introduction](#1_3)\n",
    "  - 1.4\\. [FMQA procedure](#1_4)\n",
    "- 2\\. [FMQA program implementation](#2)\n",
    "  - 2.1\\. [Random seed initialization](#2_1)\n",
    "  - 2.2\\. [Configuration of Amplify client](#2_2)\n",
    "  - 2.3\\. [Implementing FM with PyTorch](#2_3)\n",
    "  - 2.4\\. [Construction of initial training data](#2_4)\n",
    "  - 2.5\\. [Execution class for FMQA cycle](#2_5)\n",
    "- 3\\. [FMQA execution example](#3)\n",
    "  - 3.1\\. [Optimization for quadratic expressions of $\\boldsymbol{x}$](#3_1)\n",
    "  - 3.2\\. [Transition of objective function values during FMQA optimization process](#3_2)\n",
    "  - 3.3\\. [Example output from this FMQA sample program](#3_3)\n",
    "  - 3.4\\. [Summary](#3_4)\n",
    "  - 3.5\\. [Appendix](#3_5)\n",
    "- 4\\. [References](#4)\n",
    "\n",
    "$^*$ FMQA may be inappropriate when an Ising machine other than quantum annealing machine is used for the search. However, since the methodology for this type of black-box optimization is consistent regardless of the type of Ising machines used, \"FMQA\" will be used throughout this tutorial. Note that by using *Amplify* it is straightforward to switch between various annealing machines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1\\. Introduction to FMQA\n",
    "\n",
    "<a id=\"1_1\"></a>\n",
    "### 1.1\\. Black-box optimization\n",
    "\n",
    "FMQA is a black-box optimization method similar to Bayesian optimization. Usually, in mathematical optimization, the objective is to estimate a decision variable $\\boldsymbol{x}$ such that the objective function $f(\\boldsymbol{x})$ in interest is minimized (or maximized). Here, if information about the objective function $f(\\boldsymbol{x})$ (functional form, gradient, submodularity, convexity, etc.) is given, efficient optimization can be performed.\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    \\mathrm{Minimize}&\\,\\,f(\\boldsymbol{x}) \\\\\n",
    "    \\mathrm{subject\\,\\,to\\,\\,}&\\boldsymbol{x} \\in [0,1]^D\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For example, suppose the function $f(\\boldsymbol{x})$ is known (and is quadratic in $\\boldsymbol{x}$), as in some optimization problems shown in the Amplify demo tutorial. In such a case, $f(\\boldsymbol{x})$ can be used as the objective function to perform the optimization directly as a quadratic unconstrained binary optimization (QUBO: Quadratic Unconstrained Binary Optimization) problem.\n",
    "\n",
    "Here, a binary variable vector with a size $D$ is assumed for $\\boldsymbol{x}$. However, non-binary variables can be used in FMQA by using one-hot encoding for example. Such an example can be found in [Black-Box Optimization of Airfoil Geometry with Fluid Flow Simulation](https://amplify.fixstars.com/en/demo/fmqa_3_aerofoil).\n",
    "\n",
    "On the other hand, in the case of optimization to minimize (or maximize) values obtained by simulation or experiment for physical or social phenomena, the objective function $f(\\boldsymbol{x})$ corresponds to simulation or experiment, and the function cannot be described explicitly. Mathematical optimization for such an unknown objective function $f(\\boldsymbol{x})$ is called black-box optimization.\n",
    "\n",
    "In addition, evaluating such an objective function (running simulations or experiments) is usually relatively expensive (in terms of time and money, etc). Therefore, even if the set of decision variables is finite, optimization by full search is generally difficult. Therefore, an optimization method with as few objective function evaluations as possible is required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_2\"></a>\n",
    "### 1.2\\. Bayesian optimization\n",
    "\n",
    "In Bayesian optimization, black-box optimization is performed by repeating the following optimization cycle.\n",
    "\n",
    "1. Construct an acquisition function $g(\\boldsymbol{x})$ from the training data.\n",
    "1. Estimate the point $\\hat{\\boldsymbol{x}}$ where the acquisition function $g(\\boldsymbol{x})$ is minimized.\n",
    "1. Add the evaluation result $(\\hat{\\boldsymbol{x}}, \\hat{y})$ of the objective function $\\hat{y} = f(\\hat{\\boldsymbol{x}})$ to the training data\n",
    "\n",
    "By repeating this cycle, the prediction accuracy of the acquisition function $g(\\boldsymbol{x})$ improves near the optimization point, and as a result, the resulting $\\hat{\\boldsymbol{x}}$ is expected to be close to the true decision variable that minimizes the objective function $f(\\boldsymbol{x})$. However, there are following two challenges in the Bayesian optimization cycle:\n",
    "\n",
    "1. Construct an acquisition function $g(\\boldsymbol{x})$\n",
    "1. Estimate $\\hat{\\boldsymbol{x}}$ to minimize the acquisition function\n",
    "\n",
    "\n",
    "FMQA, described below, is a general framework that solves these two challenges in Bayesian optimization and implements black-box optimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_3\"></a>\n",
    "### 1.3\\. FMQA introduction\n",
    "\n",
    "Consider using the following Factorization Machine (FM), a type of machine learning model, as the acquisition function $g(\\boldsymbol{x})$ required in Bayesian optimization.\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "  g(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= w_0 + \\langle \\boldsymbol{w}, \\boldsymbol{x}\\rangle + \\sum_{i=1}^D \\sum_{j=i+1}^D \\langle \\boldsymbol{v}_i, \\boldsymbol{v}_j \\rangle x_i x_j \\\\\n",
    " &=w_0 + \\sum_{i=1}^D w_i x_i + \\sum_{i=1}^D \\sum_{j=i+1}^D \\sum_{f=1}^k v_{if}v_{jf}x_ix_j \\\\\n",
    " &=w_0 + \\sum_{i=1}^D w_i x_i + \\frac{1}{2}\\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^D v_{i f} x_i\\right)^2 - \\sum_{i=1}^D v_{i f}^2 x_i^2\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since FM is quadratic in $\\boldsymbol{x}$, the above equation yields a functional form that can be optimized by QUBO. The parameters, $\\boldsymbol{w}$ and $\\boldsymbol{v}$ ($v_{ij}$, $w_i$), in the equation, are FM parameters (weights or biases in a machine learning context) obtained after training the model in the above equation, and $k$ is a hyperparameter.\n",
    "\n",
    "The number of FM parameters depends on the hyperparameter $k$. When $k=D$, FM has the same degrees of freedom as the QUBO interaction terms, while a smaller $k$ has the effect of reducing the number of FM parameters and suppressing overlearning.\n",
    "\n",
    "Thus, using FM as the acquisition function $g(\\boldsymbol{x})$ and performing its optimization by using quantum annealing (QA) or Ising machines solves the aforementioned issues and can be applied to general problems. This black-box optimization method that combines quantum annealing and Ising machines with machine learning is called FMQA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_4\"></a>\n",
    "### 1.4\\. FMQA procedure\n",
    "\n",
    "The FMQA procedure is similar to the Bayesian optimization cycle described above as follows:\n",
    "\n",
    "First, the number of objective function evaluations $N$ that can be performed during the optimization process is determined based on the cost (time, money, etc) required for the evaluation and the available resources. For example, if an objective function evaluation (experiment or simulation) takes one hour, and the FMQA optimization must be completed in one day, the maximum number of evaluations is considered to be $N=24$. Then, we determine the number of initial training data samples $N_0$ such that $N_0<N$, and prepare the initial training data as follows. Finally, we run the FMQA cycle for $N-N_0$ times.\n",
    "\n",
    "- Preparation of initial training data ($N_0$ samples)  \n",
    "  1. Prepare $N_0$ input samples $\\{\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\cdots, \\boldsymbol{x}_{N_0}\\}$ and the corresponding $N_0$ outputs $\\{f(\\boldsymbol{x}_1 ), f(\\boldsymbol{x}_2), \\cdots, \\boldsymbol{x}_{N_0}\\}$ as initial training data. \n",
    "\n",
    "- FMQA optimization cycle ($N-N_0$ times)  \n",
    "  1. Train the FM model using the (most recent) training data and obtain the FM parameters $(\\boldsymbol{v}, \\boldsymbol{w})$.  \n",
    "  1. Estimate the input $\\hat{\\boldsymbol{x}}$ that minimizes the acquisition function $g(\\boldsymbol{x})$ by using Amplify.  \n",
    "  1. Evaluate the objective function $f(\\boldsymbol{x})$ with $\\hat{\\boldsymbol{x}}$ to obtain $\\hat{y} = f(\\hat{\\boldsymbol{x}})$. \n",
    "  1. Add $(\\hat{\\boldsymbol{x}}, \\hat{y})$ to the training data.\n",
    "\n",
    "    Repeat steps 1-4 above for $N-N_0$ times.\n",
    "\n",
    "As the FMQA process progresses, the prediction accuracy of the FM is expected to improve near the optimization point and a better estimate of $\\hat{\\boldsymbol{x}}$ is expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2\\. FMQA program implementation\n",
    "\n",
    "In this section, the FMQA program is implemented as follows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_1\"></a>\n",
    "### 2.1\\. Random seed initialization\n",
    "\n",
    "We define a function `seed_everything()` to initialize random seed values to ensure that the machine learning results do not change with each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_2\"></a>\n",
    "### 2.2\\. Configuration of Amplify client\n",
    "\n",
    "Here, we create an Amplify client and set the necessary parameters. In the following, we set the timeout for a single search by the Ising machine to 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify.client import FixstarsClient\n",
    "\n",
    "client = FixstarsClient()\n",
    "client.parameters.timeout = 1000  # Timeout 1s\n",
    "# client.token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # If you use Amplify in a local environment, enter the Amplify API token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_3\"></a>\n",
    "### 2.3\\. Implementing FM with PyTorch\n",
    "\n",
    "In this example code, FM is implemented with PyTorch. In the `TorchFM` class, we define the acquisition function $g(\\boldsymbol{x})$ as a machine learning model. Each term in $g(\\boldsymbol{x})$ corresponds directly to `out_lin`, `out_1`, `out_2`, and `out_inter` in the `TorchFM` class, as in the following equation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  g(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= \n",
    "  \\underset{\\color{red}{\\mathtt{out\\_lin}}}{\\underline{ w_0 + \\sum_{i=1}^D w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_inter}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^D v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^D v_{i f}^2 x_i^2 }} \\right] }}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int):\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(d, k), requires_grad=True)\n",
    "        self.lin = nn.Linear(\n",
    "            d, 1\n",
    "        )  # The first and second terms on the right-hand side are fully connected network\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)\n",
    "        out_inter = 0.5 * (out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a function `train()` is defined to train the FM based on the training data sets. As in general machine learning, this function divides the data sets into training data and validation data, then optimizes the FM parameters using the training data, and validates the model during training using the validation data. The `train()` function returns the model with the highest prediction accuracy for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    model_class=None,\n",
    "    model_params=None,\n",
    "    batch_size=1024,\n",
    "    epochs=3000,\n",
    "    criterion=None,\n",
    "    optimizer_class=None,\n",
    "    opt_params=None,\n",
    "    lr_sche_class=None,\n",
    "    lr_sche_params=None,\n",
    "):\n",
    "    X_tensor, y_tensor = (\n",
    "        torch.from_numpy(X).float(),\n",
    "        torch.from_numpy(y).float(),\n",
    "    )\n",
    "    indices = np.array(range(X.shape[0]))\n",
    "    indices_train, indices_valid = train_test_split(\n",
    "        indices, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = TensorDataset(X_tensor[indices_train], y_tensor[indices_train])\n",
    "    valid_set = TensorDataset(X_tensor[indices_valid], y_tensor[indices_valid])\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        \"valid\": DataLoader(valid_set, batch_size=batch_size, shuffle=False),\n",
    "    }\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    optimizer = optimizer_class(model.parameters(), **opt_params)\n",
    "    if lr_sche_class is not None:\n",
    "        scheduler = lr_sche_class(optimizer, **lr_sche_params)\n",
    "    best_score = 1e18\n",
    "    for epoch in range(epochs):\n",
    "        losses = {\"train\": 0.0, \"valid\": 0.0}\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for batch_x, batch_y in loaders[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch_x).T[0]\n",
    "                loss = criterion(out, batch_y)\n",
    "                losses[phase] += loss.item() * batch_x.size(0)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "            losses[phase] /= len(loaders[phase].dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            if best_score > losses[\"valid\"]:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_score = losses[\"valid\"]\n",
    "        if lr_sche_class is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_4\"></a>\n",
    "### 2.4\\. Construction of initial training data\n",
    "\n",
    "The `gen_training_data` function evaluates the objective function $f(\\boldsymbol{x})$ against the input value $\\boldsymbol{x}$ to produce $N_0$ input-output pairs (initial training data). The input value $\\boldsymbol{x}$ can be determined in a variety of ways, such as by using a random number or a value suitable for machine learning based on prior knowledge. You can also build up the training data from the results of previous experiments or simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(D: int, N0: int, true_func):\n",
    "    assert N0 < 2**D\n",
    "    # N0 input values are obtained using random numbers\n",
    "    X = np.random.randint(0, 2, size=(N0, D))\n",
    "    # Remove duplicate input values and add new input values using random numbers\n",
    "    X = np.unique(X, axis=0)\n",
    "    while X.shape[0] != N0:\n",
    "        X = np.vstack((X, np.random.randint(0, 2, size=(N0 - X.shape[0], D))))\n",
    "        X = np.unique(X, axis=0)\n",
    "    y = np.zeros(N0)\n",
    "    # Obtain output values corresponding to N0 input values by evaluating the objective function, true_func\n",
    "    for i in range(N0):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Generating {i}-th training data set.\")\n",
    "        y[i] = true_func(X[i])\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_5\"></a>\n",
    "### 2.5\\. Execution class for FMQA cycle\n",
    "\n",
    "`FMQA.cycle()` executes an FMQA cycle that is performed for $N-N_0$ times using the pre-prepared initial training data. `FMQA.step()` is a function that executes only one FMQA cycle, and is called $N-N_0$ times by `FMQA.cycle()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import (\n",
    "    Solver,\n",
    "    BinarySymbolGenerator,\n",
    "    sum_poly,\n",
    "    BinaryMatrix,\n",
    "    BinaryQuadraticModel,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "class FMQA:\n",
    "    def __init__(self, D: int, N: int, N0: int, k: int, true_func, solver) -> None:\n",
    "        assert N0 < N\n",
    "        self.D = D\n",
    "        self.N = N\n",
    "        self.N0 = N0\n",
    "        self.k = k\n",
    "        self.true_func = true_func\n",
    "        self.solver = solver\n",
    "        self.y = None\n",
    "\n",
    "    # A member function that repeatedly performs (N-N0)x FMQA based on the training data with adding new training data\n",
    "    def cycle(self, X, y, log=False) -> np.ndarray:\n",
    "        print(f\"Starting FMQA cycles...\")\n",
    "        pred_x = X[0]\n",
    "        pred_y = 1e18\n",
    "        for i in range(self.N - self.N0):\n",
    "            print(f\"FMQA Cycle #{i} \", end=\"\")\n",
    "            try:\n",
    "                x_hat = self.step(X, y)\n",
    "            except RuntimeError:\n",
    "                sys.exit(f\"Unknown error, i = {i}\")\n",
    "            # If an input value identical to the found x_hat already exists in the current training data set, a neighboring value is used as a new x_hat.\n",
    "            is_identical = True\n",
    "            while is_identical:\n",
    "                is_identical = False\n",
    "                for j in range(i + self.N0):\n",
    "                    if np.all(x_hat == X[j, :]):\n",
    "                        change_id = np.random.randint(0, self.D, 1)\n",
    "                        x_hat[change_id.item()] = 1 - x_hat[change_id.item()]\n",
    "                        if log:\n",
    "                            print(f\"{i=}, Identical x is found, {x_hat=}\")\n",
    "                        is_identical = True\n",
    "                        break\n",
    "            # Evaluate objective function f() with x_hat\n",
    "            y_hat = self.true_func(x_hat)\n",
    "            # Add an input-output pair [x_hat, y_hat] to the training data set\n",
    "            X = np.vstack((X, x_hat))\n",
    "            y = np.append(y, y_hat)\n",
    "            # Copy the input-output pair to [pred_x, pred_y] when the evaluated value of the objective function updates the minimum value\n",
    "            if pred_y > y_hat:\n",
    "                pred_y = y_hat\n",
    "                pred_x = x_hat\n",
    "                print(f\"variable updated, {pred_y=}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "            # Exit the \"for\" statement if all inputs have been fully explored\n",
    "            if len(y) >= 2**self.D:\n",
    "                print(f\"Fully searched at {i=}. Terminating FMQA cycles.\")\n",
    "                break\n",
    "        self.y = y\n",
    "        return pred_x\n",
    "\n",
    "    # Member function to perform one FMQA cycle\n",
    "    def step(self, X, y) -> np.ndarray:\n",
    "        # Train FM\n",
    "        model = train(\n",
    "            X,\n",
    "            y,\n",
    "            model_class=TorchFM,\n",
    "            model_params={\"d\": self.D, \"k\": self.k},\n",
    "            batch_size=8,\n",
    "            epochs=2000,\n",
    "            criterion=nn.MSELoss(),\n",
    "            optimizer_class=torch.optim.AdamW,\n",
    "            opt_params={\"lr\": 1},\n",
    "        )\n",
    "        # Extract FM parameters from the trained FM model\n",
    "        v, w, w0 = list(model.parameters())\n",
    "        v = v.detach().numpy()\n",
    "        w = w.detach().numpy()[0]\n",
    "        w0 = w0.detach().numpy()[0]\n",
    "        # Solve a QUBO problem using a quantum annealing or Ising machine\n",
    "        gen = BinarySymbolGenerator()  # Declare a variable generator, BinaryPoly\n",
    "        q = gen.array(self.D)  # Generate decision variables using BinaryPoly\n",
    "        cost = self.__FM_as_QUBO(\n",
    "            q, w0, w, v\n",
    "        )  # Define FM as a QUBO equation from FM parameters\n",
    "        result = self.solver.solve(\n",
    "            cost\n",
    "        )  # Pass the objective function to Amplify solver\n",
    "        if len(result.solutions) == 0:\n",
    "            raise RuntimeError(\"No solution was found.\")\n",
    "        values = result.solutions[0].values\n",
    "        q_values = q.decode(values)\n",
    "        return q_values\n",
    "\n",
    "    # A function that defines FM as a QUBO equation from FM parameters. As with the previously defined TorchFM class, the formula is written as per the acquisition function form of g(x).\n",
    "    def __FM_as_QUBO(self, x, w0, w, v):\n",
    "        lin = w0 + (x.T @ w)\n",
    "        D = w.shape[0]\n",
    "        out_1 = sum_poly(self.k, lambda i: sum_poly(D, lambda j: x[j] * v[j, i]) ** 2)\n",
    "        # Note that x[j] = x[j]^2 since x[j] is a binary variable in the following equation.\n",
    "        out_2 = sum_poly(\n",
    "            self.k, lambda i: sum_poly(D, lambda j: x[j] * v[j, i] * v[j, i])\n",
    "        )\n",
    "        return lin + (out_1 - out_2) / 2\n",
    "\n",
    "    \"\"\"The sum_poly used in __FM_as_QUBO above is inefficient in terms of computation speed and memory. In the case of FM, \n",
    "    where the interaction terms of the decision variables are generally nonzero, the following implementation using BinaryMatrix \n",
    "    is more efficient. Here, the quadratic terms in BinaryMatrix correspond to the non-diagonal terms represented by the upper \n",
    "    triangular matrix, so x(1/2) for the quadratic terms in the FM formula is unnecessary. Also, although x is taken as an \n",
    "    argument just to match the function signature with __FM_as_QUBO above (implementation using sum_poly), it is not needed in \n",
    "    this implementation using BinaryMatrix.\n",
    "    def __FM_as_QUBO(self, x, w0, w, v):\n",
    "        out_1_matrix = v @ v.T\n",
    "        out_2_matrix = np.diag((v * v).sum(axis=1))\n",
    "        matrix = BinaryMatrix(out_1_matrix - out_2_matrix + np.diag(w))\n",
    "        # Do not forget to put the constant term w0 in the second argument of BinaryQuadraticModel.\n",
    "        model = BinaryQuadraticModel(matrix, w0)\n",
    "        return model\n",
    "    \"\"\"\n",
    "\n",
    "    # A function to plot the history of i-th objective function evaluations performed within the initial training data construction (blue) and during FMQA cycles (red).\n",
    "    def plot_history(self):\n",
    "        assert self.y is not None\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        plt.plot(\n",
    "            [i for i in range(self.N0)],\n",
    "            self.y[: self.N0],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"b\",\n",
    "        )  # Objective function evaluation values at the time of initial training data generation (random process)\n",
    "        plt.plot(\n",
    "            [i for i in range(self.N0, self.N)],\n",
    "            self.y[self.N0 :],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"r\",\n",
    "        )  # Objective function evaluation values during the FMQA cycles (FMQA cycle process)\n",
    "        plt.xlabel(\"i-th evaluation of f(x)\", fontsize=18)\n",
    "        plt.ylabel(\"f(x)\", fontsize=18)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3\\. FMQA execution example\n",
    "\n",
    "<a id=\"3_1\"></a>\n",
    "### 3.1\\. Optimization for quadratic expressions of $\\boldsymbol{x}$\n",
    "\n",
    "Let us perform black-box optimization using FMQA. While FMQA is usually applied for an objective function that is black-box and expensive to evaluate, this tutorial considers the following algebraic expression as the objective function for simplicity and for explanation.\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x}) = \\boldsymbol{x}^T Q \\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "Here, $Q$ is a $d\\times d$ matrix, whose components have zero mean, and they are generated by the random numbers as defined in `make_Q`. The above $f(\\boldsymbol{x})$, which is a known function, is treated as an unknown function (=black-box).\n",
    "\n",
    "Also, note that under the following conditions ($D=100$, $N=100$, $N_0=70$), it will take several minutes to complete all FMQA cycles. An example output is shown in \"[3.3\\. Example output from this FMQA sample program](#3_3)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a d-dimensional symmetric matrix whose components have zero mean\n",
    "def make_Q(d) -> np.ndarray:\n",
    "    Q_true = np.random.rand(d, d)\n",
    "    Q_true = (Q_true + Q_true.T) / 2\n",
    "    Q_true = Q_true - np.mean(Q_true)\n",
    "    return Q_true\n",
    "\n",
    "\n",
    "# Initialize random seed values\n",
    "seed_everything(0)\n",
    "\n",
    "# Size of input values (problem size)\n",
    "D = 100\n",
    "# Matrix Q used in the \"true function\"\n",
    "Q = make_Q(D)\n",
    "\n",
    "\n",
    "def true_func(x):\n",
    "    # Definition of the objective function (xQx).\n",
    "    # Essentially, cost is the result value of the unknown function (simulation or experiment) or the result value of the subsequent process.\n",
    "    cost = x @ Q @ x\n",
    "    return cost\n",
    "\n",
    "\n",
    "N = 70  # Number of times the function can be evaluated\n",
    "N0 = 60  # Number of samples of initial training data\n",
    "k = 10  # Dimension of the vector in FM (hyperparameters)\n",
    "\n",
    "\n",
    "# client: Amplify client created earlier\n",
    "solver = Solver(client)\n",
    "# Generate initial training data\n",
    "X, y = gen_training_data(D, N0, true_func)\n",
    "\n",
    "# Instantiate FMQA class\n",
    "fmqa_solver = FMQA(D, N, N0, k, true_func, solver)\n",
    "# Run FMQA cycle\n",
    "pred_x = fmqa_solver.cycle(X, y)\n",
    "# Output optimization results\n",
    "print(\"pred x:\", pred_x)\n",
    "print(\"pred value:\", true_func(pred_x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2\"></a>\n",
    "### 3.2\\. Transition of objective function values during the FMQA optimization process\n",
    "\n",
    "The following line displays the evolution of the objective function values during the FMQA optimization process. The initial $N_0$ objective function values (blue line) are obtained from randomly generated input values during initial training data generation. The following red line shows the objective function values during the $N-N_0$ FMQA optimization cycles.\n",
    "\n",
    "The input value $\\hat{x}$ obtained from the FMQA optimization cycles shows that the minimum value of the objective function is successively updated (see the output example in \"[3.3\\. Example output from this FMQA sample program](#3_3)\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fmqa_solver.plot_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_3\"></a>\n",
    "### 3.3\\. Example output from this FMQA sample program\n",
    "\n",
    "In general, due to the principle of the heuristic algorithm used in `FixstarsClient`, the solutions obtained are not completely reproducible, but typical standard output and image output obtained when running this sample code are shown below. The values obtained may vary slightly from run to run.\n",
    "\n",
    "- When the FMQA code described in [\"3.1. Optimization for quadratic expressions of $\\boldsymbol{x}$\"](#3_1) is run under the given conditions, the following standard output is sequentially produced.\n",
    "\n",
    "    ```shell\n",
    "    Generating 0-th training data set.\n",
    "    Generating 10-th training data set.\n",
    "    Generating 20-th training data set.\n",
    "    Generating 30-th training data set.\n",
    "    Generating 40-th training data set.\n",
    "    Generating 50-th training data set.\n",
    "    Starting FMQA cycles...\n",
    "    FMQA Cycle #0 variable updated, pred_y=-59.15752919611154\n",
    "    FMQA Cycle #1 \n",
    "    FMQA Cycle #2 variable updated, pred_y=-72.66802296872575\n",
    "    FMQA Cycle #3 \n",
    "    FMQA Cycle #4 \n",
    "    FMQA Cycle #5 \n",
    "    FMQA Cycle #6 \n",
    "    FMQA Cycle #7 \n",
    "    FMQA Cycle #8 variable updated, pred_y=-76.81540215271143\n",
    "    FMQA Cycle #9 \n",
    "    pred x: [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
    "    1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
    "    2. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
    "    3. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
    "    4. 0. 0. 1.]\n",
    "    pred value: -76.81540215271143\n",
    "    ```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The output image of `fmqa_solver.plot_history()` described in [\"3.2\\. Transition of objective function values during FMQA optimization process\"](#3_2) is shown below.\n",
    "\n",
    "    ![plot_history](../figures/fmqa_0_algebra_plot_history.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_4\"></a>\n",
    "### 3.4\\. Summary\n",
    "\n",
    "In this tutorial, so-called FMQA optimization was performed on a relatively simple known function. Amplify also provides examples and sample code for more realistic model cases.\n",
    "\n",
    "- [Black-Box Optimization Exploration of Model Superconducting Materials](https://amplify.fixstars.com/en/demo/fmqa_1_supercon)\n",
    "- [Black-Box Optimization of Operating Condition in a Chemical Reactor](https://amplify.fixstars.com/en/demo/fmqa_2_reactor)\n",
    "- [Black-Box Optimization of Airfoil Geometry by Fluid Flow Simulation](https://amplify.fixstars.com/en/demo/fmqa_3_aerofoil)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_5\"></a>\n",
    "### 3.5\\. Appendix\n",
    "\n",
    "Since $f(x) = x^{\\top}Qx$ is a quadratic equation and the formula is known, it is possible to search for optimal input values directly by quantum annealing or Ising machines without using FMQA. The code below optimizes this function directly using QUBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a variable generator, BinaryPoly\n",
    "gen = BinarySymbolGenerator()\n",
    "# Create 1D array of decision variables with size D\n",
    "q = gen.array(D)\n",
    "# Formulate xQx as the objective function of QUBO\n",
    "cost = sum_poly(D, lambda i: sum_poly(D, lambda j: Q[i, j] * q[i] * q[j]))\n",
    "# Pass the objective function to Amplify for solution seeking.\n",
    "result = solver.solve(cost)\n",
    "if len(result.solutions) == 0:\n",
    "    raise RuntimeError(\"No solution was found.\")\n",
    "# Extract and display the estimated optimal solution.\n",
    "values = result.solutions[0].values\n",
    "true_x = q.decode(values)\n",
    "print(\"true x:\", true_x)\n",
    "print(\"true value:\", true_func(true_x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4\\. References\n",
    "\n",
    "The present black-box optimization method that combines quantum annealing and Ising machines with machine learning is called FMQA, which has been originally proposed as FMQA in the following research.\n",
    "\n",
    "- K. Kitai, J. Guo, S. Ju, S. Tanaka, K. Tsuda, J. Shiomi, and R. Tamura,\n",
    "\"Designing metamaterials with quantum annealing and factorization machines\", \n",
    "[Physical Review Research 2, 013319 (2020)](https://doi.org/10.1103/PhysRevResearch.2.013319).\n",
    "\n",
    "In this study, the search for \"metamaterials\" is carried out using FMQA, which also have shown superior performance compared to Bayesian optimization, a conventional black-box optimization method. \n",
    "\n",
    "In the following study, the same black-box optimization method is also applied to the design of photonic crystals.\n",
    "\n",
    "- T. Inoue, Y. Seki, S. Tanaka, N. Togawa, K. Ishizaki, and S. Noda, \"Towards optimization of photonic-crystal surface-emitting lasers via quantum annealing,\" [Opt. Express  30, 43503-43512 (2022)](https://doi.org/10.1364/OE.476839). \n",
    "\n",
    "These studies suggest that this optimization method (FMQA), based on FM and combinatorial optimization, may have general applicability in black-box optimization problems in various fields. In Fixstars Amplify, there are several examples of such black-box optimization in the areas of chemical reaction, fluid dynamics, as well as material search, as follows:\n",
    "\n",
    "- [Black-Box Optimization Exploration of Model Superconducting Materials](https://amplify.fixstars.com/en/demo/fmqa_1_supercon)\n",
    "- [Black-Box Optimization of Operating Condition in a Chemical Reactor](https://amplify.fixstars.com/en/demo/fmqa_2_reactor)\n",
    "- [Black-Box Optimization of Airfoil Geometry by Fluid Flow Simulation](https://amplify.fixstars.com/en/demo/fmqa_3_aerofoil)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
