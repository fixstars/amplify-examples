{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-Box Optimization of Model Superconducting Materials\n",
    "\n",
    "\n",
    "To illustrate the effective use of black-box optimization, this sample code describes the optimization for superconducting materials composed of pseudo-materials as an example problem.\n",
    "\n",
    "Although the present sample code performs material searches based on nonlinear algebraic models, it is possible to perform black-box optimization in the same steps using high-precision simulations or experimental measurement results instead of model algebraic expressions. Even in such cases, you can use this example almost as is.\n",
    "\n",
    "For a basic introduction to black-box optimization and FMQA, see \"[Black-Box Optimization with Quantum Annealing and Ising Machines](https://amplify.fixstars.com/en/demo/fmqa_0_algebra)\".\n",
    "\n",
    "Also, more applied, complex optimization problems using black-box optimization are explained in:\n",
    "\n",
    "- [Black-Box Optimization of Operating Condition in a Chemical Reactor](https://amplify.fixstars.com/en/demo/fmqa_2_reactor)\n",
    "- [Black-Box Optimization of Airfoil Geometry by Fluid Flow Simulation](https://amplify.fixstars.com/en/demo/fmqa_3_aerofoil)\n",
    "\n",
    "\n",
    "This notebook contains the following sections:\n",
    "\n",
    "- 1\\. [Problem setting](#1)\n",
    "  - 1.1\\. [Search scenario for superconducting materials](#1_1)\n",
    "  - 1.2\\. [Random number initialization](#1_2)\n",
    "  - 1.3\\. [Definition of critical temperature model](#1_3)\n",
    "- 2\\. [FMQA program implementation](#2)\n",
    "  - 2.1\\. [Configuration of Amplify client](#2_1)\n",
    "  - 2.2\\. [Implementing FM with PyTorch](#2_2)\n",
    "  - 2.3\\. [Construction of initial training data](#2_3)\n",
    "  - 2.4\\. [Execution class for FMQA cycle](#2_4)\n",
    "- 3\\. [FMQA execution example](#3)\n",
    "  - 3.1\\. [Material search for the highest critical temperature](#3_1)\n",
    "  - 3.2\\. [Transition of objective function values during FMQA optimization process](#3_2)\n",
    "  - 3.3\\. [Example output from the FMQA sample program](#3_3)\n",
    "- [Exercises](#4)\n",
    "  - [Exercise 1](#4_1)\n",
    "  - [Exercise 2](#4_2)\n",
    "  - [Exercise 3](#4_3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1\\. Problem setting\n",
    "\n",
    "<a id=\"1_1\"></a>\n",
    "### 1.1\\. Search scenario for superconducting materials\n",
    "\n",
    "Superconductivity technology is expected to be utilized in the fields of transportation, such as maglev trains, metrology, and energy. Various superconducting materials are currently being developed to realize superconductivity.\n",
    "\n",
    "The temperature at which superconductivity is achieved (critical temperature) is generally around the absolute temperature of 0 K (Kelvin) for currently confirmed superconducting materials. Because of this, superconductivity requires costly cooling to be exploited, and its application in the real world is currently limited. Therefore, the search for high-temperature superconductors is a pressing issue.\n",
    "\n",
    "Typically, the search for materials that realize superconductivity involves a trial-and-error process of selecting and synthesizing several materials, repeatedly evaluating the critical temperature of the synthesized materials by measurement, and identifying the material to be synthesized that achieves a higher critical temperature. This process of synthesis and critical temperature evaluation is considered time-consuming. For this search, a black box optimization method is used to find a combination of materials close to the optimal solution with a relatively small number of evaluations.\n",
    "\n",
    "In this example, the search for superconducting materials consisting of pseudo materials is treated as an example to illustrate the material search by a black-box optimization method (FMQA), and a critical temperature model is used to evaluate the critical temperature. Note that the critical temperature models presented below and the combinations of materials obtained are not necessarily physically accurate, and thus the example serves for illustration purposes only."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_2\"></a>\n",
    "### 1.2\\. Random number initialization\n",
    "\n",
    "We define a function `seed_everything()` to initialize random seed values to ensure that the machine learning results do not change with each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_3\"></a>\n",
    "### 1.3\\. Definition of critical temperature model\n",
    "\n",
    "This example code selects a combination of several materials from $D$ types of materials and performs an optimization to maximize the critical temperature of the superconducting material produced by their synthesis.\n",
    "\n",
    "In general, the critical temperature can be evaluated by experimental measurement, which requires a relatively large cost (time and money) each time the evaluation is performed.\n",
    "\n",
    "In this example code, instead of measuring the critical temperature, the following critical temperature model `supercon_temperature()` is used for evaluation. However, this function is only a substitute for experimental measurement, and its contents and parameters are treated as unknown, and the number of calls to `supercon_temperature()` is also treated as limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# A function to randomly generate various coefficient tables for critical temperature calculations.\n",
    "\n",
    "\n",
    "def set_properties(size):\n",
    "    mu, sigma, ratio = 0.0, 1.0, 0.2\n",
    "    table1 = np.random.rand(size) * 1e5 * (0.1 * math.log(size) - 0.23)\n",
    "    table2 = np.random.lognormal(mu, sigma, size) * ratio\n",
    "    table3 = np.random.lognormal(mu, sigma, size) * ratio\n",
    "    return table1, table2, table3\n",
    "\n",
    "\n",
    "# A model function which calculates the critical temperature of the superconducting material to be synthesized from a given combination of materials x (a one-dimensional array of numpy) and the physical properties of each material.\n",
    "\n",
    "\n",
    "def supercon_temperature(x, debye_table, state_table, interaction_table):\n",
    "    debye_temperature = np.sum(x * debye_table) / np.sum(x)\n",
    "    state_density = np.sum(x * state_table) / np.sum(x)\n",
    "    interaction = np.sum(x * interaction_table) / np.sum(x)\n",
    "    crit_temp = debye_temperature * math.exp(-1.0 / state_density / interaction)\n",
    "    return crit_temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the model `supercon_temperature(x)` for the critical temperature defined above is used to evaluate the critical temperature of superconducting materials synthesized from a random selection of materials. Here, `D` is the number of materials to be selected, and the input `x` is a vector of size `D` consisting of 0 or 1.\n",
    "\n",
    "For example, in the case of selecting the first and last of five materials to be combined, the input vector would be `x = [1, 0, 0, 0, 0, 1]`. In this case, there are $2^5-1=31$ possible choices (combinations).\n",
    "\n",
    "For `D = 100`, the number of combinations is approximately $10^{30}$, and the full-search method is considered difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random seed values\n",
    "seed_everything()\n",
    "\n",
    "# Size of input values (the number of pseudo-materials)\n",
    "D = 100\n",
    "\n",
    "# Property tables\n",
    "debye_temperature_table, state_density_table, interaction_table = set_properties(D)\n",
    "\n",
    "# Random searches: evaluate the supercon_temp() function for n_cycle times with random input x and output the obtained maximum and average critical temperatures.\n",
    "n_cycle = 100\n",
    "t_max = 0.0  # Variable to store the maximum value of critical temperature.\n",
    "t_mean = 0.0  # Variable to calculate the average value of the critical temperature\n",
    "for i in range(n_cycle):\n",
    "    x = np.random.randint(0, 2, D)\n",
    "    if np.sum(x) == 0:\n",
    "        continue\n",
    "    t_c = supercon_temperature(\n",
    "        x, debye_temperature_table, state_density_table, interaction_table\n",
    "    )\n",
    "    if t_max < t_c:\n",
    "        t_max = t_c\n",
    "    t_mean += t_c\n",
    "t_mean /= n_cycle\n",
    "\n",
    "print(f\"Max. critical temperature: {t_max:.2f} K\")\n",
    "print(f\"Mean critical temperature: {t_mean:.2f} K\")\n",
    "print(f\"{n_cycle=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2\\. FMQA program implementation\n",
    "\n",
    "This section describes the program implementation of FMQA, which is identical to the implementation in \"[Black-Box Optimization with Quantum Annealing and Ising Machines](https://amplify.fixstars.com/en/demo/fmqa_0_algebra)\", so please refer to that for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_1\"></a>\n",
    "### 2.1\\. Configuration of Amplify client\n",
    "\n",
    "Here, we create an Amplify client and set the necessary parameters. In the following, we set the timeout for a single search by the Ising machine to 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify.client import FixstarsClient\n",
    "\n",
    "client = FixstarsClient()\n",
    "client.parameters.timeout = 1000  # Timeout 1s\n",
    "# client.token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # If you use Amplify in a local environment, enter the Amplify API token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_2\"></a>\n",
    "### 2.2\\. Implementing FM with PyTorch\n",
    "\n",
    "Here, FM is implemented with PyTorch. In the `TorchFM` class, we define the acquisition function $g(x)$ as a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int):\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(d, k), requires_grad=True)\n",
    "        self.lin = nn.Linear(\n",
    "            d, 1\n",
    "        )  # The first and second terms on the right-hand side are fully connected network\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)\n",
    "        out_inter = 0.5 * (out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, a function `train()` is defined to train the FM based on the training data sets. As in general machine learning methods, this function divides the data sets into training data and validation data, then optimizes the FM parameters using the training data, and validates the model during training using the validation data. The `train()` function returns the model with the highest prediction accuracy for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    model_class=None,\n",
    "    model_params=None,\n",
    "    batch_size=1024,\n",
    "    epochs=3000,\n",
    "    criterion=None,\n",
    "    optimizer_class=None,\n",
    "    opt_params=None,\n",
    "    lr_sche_class=None,\n",
    "    lr_sche_params=None,\n",
    "):\n",
    "    X_tensor, y_tensor = (\n",
    "        torch.from_numpy(X).float(),\n",
    "        torch.from_numpy(y).float(),\n",
    "    )\n",
    "    indices = np.array(range(X.shape[0]))\n",
    "    indices_train, indices_valid = train_test_split(\n",
    "        indices, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = TensorDataset(X_tensor[indices_train], y_tensor[indices_train])\n",
    "    valid_set = TensorDataset(X_tensor[indices_valid], y_tensor[indices_valid])\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        \"valid\": DataLoader(valid_set, batch_size=batch_size, shuffle=False),\n",
    "    }\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    optimizer = optimizer_class(model.parameters(), **opt_params)\n",
    "    if lr_sche_class is not None:\n",
    "        scheduler = lr_sche_class(optimizer, **lr_sche_params)\n",
    "    best_score = 1e18\n",
    "    for epoch in range(epochs):\n",
    "        losses = {\"train\": 0.0, \"valid\": 0.0}\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for batch_x, batch_y in loaders[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch_x).T[0]\n",
    "                loss = criterion(out, batch_y)\n",
    "                losses[phase] += loss.item() * batch_x.size(0)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "            losses[phase] /= len(loaders[phase].dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            if best_score > losses[\"valid\"]:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_score = losses[\"valid\"]\n",
    "        if lr_sche_class is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_3\"></a>\n",
    "### 2.3\\. Construction of initial training data\n",
    "\n",
    "The `gen_training_data` function evaluates the objective function $f(x)$ against the input value $x$ to produce $N_0$​ input-output pairs (initial training data). The input value $x$ can be determined in a variety of ways, such as by using a random number or a value suitable for machine learning based on prior knowledge. You can also build up the training data from the results of previous experiments or simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(D: int, N0: int, true_func):\n",
    "    assert N0 < 2**D\n",
    "    # N0 input values are obtained using random numbers\n",
    "    X = np.random.randint(0, 2, size=(N0, D))\n",
    "    # Remove duplicate input values and add new input values using random numbers\n",
    "    X = np.unique(X, axis=0)\n",
    "    while X.shape[0] != N0:\n",
    "        X = np.vstack((X, np.random.randint(0, 2, size=(N0 - X.shape[0], D))))\n",
    "        X = np.unique(X, axis=0)\n",
    "    y = np.zeros(N0)\n",
    "    # Obtain output values corresponding to N0 input values by evaluating the objective function, true_func\n",
    "    for i in range(N0):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Generating {i}-th training data set.\")\n",
    "        y[i] = true_func(X[i])\n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_4\"></a>\n",
    "### 2.4\\. Execution class for FMQA cycle\n",
    "\n",
    "`FMQA.cycle()` executes an FMQA cycle that is performed for $N−N_0$​ times using the pre-prepared initial training data. `FMQA.step()` is a function that executes only one FMQA cycle, and is called $N−N_0$​ times by `FMQA.cycle()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import (\n",
    "    Solver,\n",
    "    BinarySymbolGenerator,\n",
    "    sum_poly,\n",
    "    BinaryMatrix,\n",
    "    BinaryQuadraticModel,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "class FMQA:\n",
    "    def __init__(self, D: int, N: int, N0: int, k: int, true_func, solver) -> None:\n",
    "        assert N0 < N\n",
    "        self.D = D\n",
    "        self.N = N\n",
    "        self.N0 = N0\n",
    "        self.k = k\n",
    "        self.true_func = true_func\n",
    "        self.solver = solver\n",
    "        self.y = None\n",
    "\n",
    "    # A member function that repeatedly performs (N-N0)x FMQA based on the training data with adding new training data\n",
    "    def cycle(self, X, y, log=False) -> np.ndarray:\n",
    "        print(f\"Starting FMQA cycles...\")\n",
    "        pred_x = X[0]\n",
    "        pred_y = 1e18\n",
    "        for i in range(self.N - self.N0):\n",
    "            print(f\"FMQA Cycle #{i} \", end=\"\")\n",
    "            try:\n",
    "                x_hat = self.step(X, y)\n",
    "            except RuntimeError:\n",
    "                sys.exit(f\"Unknown error, i = {i}\")\n",
    "            # If an input value identical to the found x_hat already exists in the current training data set, a neighboring value is used as a new x_hat.\n",
    "            is_identical = True\n",
    "            while is_identical:\n",
    "                is_identical = False\n",
    "                for j in range(i + self.N0):\n",
    "                    if np.all(x_hat == X[j, :]):\n",
    "                        change_id = np.random.randint(0, self.D, 1)\n",
    "                        x_hat[change_id.item()] = 1 - x_hat[change_id.item()]\n",
    "                        if log:\n",
    "                            print(f\"{i=}, Identical x is found, {x_hat=}\")\n",
    "                        is_identical = True\n",
    "                        break\n",
    "            # Evaluate objective function f() with x_hat\n",
    "            y_hat = self.true_func(x_hat)\n",
    "            # Add an input-output pair [x_hat, y_hat] to the training data set\n",
    "            X = np.vstack((X, x_hat))\n",
    "            y = np.append(y, y_hat)\n",
    "            # Copy the input-output pair to [pred_x, pred_y] when the evaluated value of the objective function updates the minimum value\n",
    "            if pred_y > y_hat:\n",
    "                pred_y = y_hat\n",
    "                pred_x = x_hat\n",
    "                print(f\"variable updated, {pred_y=}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "            # Exit the \"for\" statement if all inputs have been fully explored\n",
    "            if len(y) >= 2**self.D:\n",
    "                print(f\"Fully searched at {i=}. Terminating FMQA cycles.\")\n",
    "                break\n",
    "        self.y = y\n",
    "        return pred_x\n",
    "\n",
    "    # Member function to perform one FMQA cycle\n",
    "    def step(self, X, y) -> np.ndarray:\n",
    "        # Train FM\n",
    "        model = train(\n",
    "            X,\n",
    "            y,\n",
    "            model_class=TorchFM,\n",
    "            model_params={\"d\": self.D, \"k\": self.k},\n",
    "            batch_size=8,\n",
    "            epochs=2000,\n",
    "            criterion=nn.MSELoss(),\n",
    "            optimizer_class=torch.optim.AdamW,\n",
    "            opt_params={\"lr\": 1},\n",
    "        )\n",
    "        # Extract FM parameters from the trained FM model\n",
    "        v, w, w0 = list(model.parameters())\n",
    "        v = v.detach().numpy()\n",
    "        w = w.detach().numpy()[0]\n",
    "        w0 = w0.detach().numpy()[0]\n",
    "        # Solve a QUBO problem using a quantum annealing or Ising machine\n",
    "        gen = BinarySymbolGenerator()  # Declare a variable generator, BinaryPoly\n",
    "        q = gen.array(self.D)  # Generate decision variables using BinaryPoly\n",
    "        cost = self.__FM_as_QUBO(\n",
    "            q, w0, w, v\n",
    "        )  # Define FM as a QUBO equation from FM parameters\n",
    "        result = self.solver.solve(\n",
    "            cost\n",
    "        )  # Pass the objective function to Amplify solver\n",
    "        if len(result.solutions) == 0:\n",
    "            raise RuntimeError(\"No solution was found.\")\n",
    "        values = result.solutions[0].values\n",
    "        q_values = q.decode(values)\n",
    "        return q_values\n",
    "\n",
    "    # A function that defines FM as a QUBO equation from FM parameters. As with the previously defined TorchFM class, the formula is written as per the acquisition function form of g(x).\n",
    "    def __FM_as_QUBO(self, x, w0, w, v):\n",
    "        lin = w0 + (x.T @ w)\n",
    "        D = w.shape[0]\n",
    "        out_1 = sum_poly(self.k, lambda i: sum_poly(D, lambda j: x[j] * v[j, i]) ** 2)\n",
    "        # Note that x[j] = x[j]^2 since x[j] is a binary variable in the following equation.\n",
    "        out_2 = sum_poly(\n",
    "            self.k, lambda i: sum_poly(D, lambda j: x[j] * v[j, i] * v[j, i])\n",
    "        )\n",
    "        return lin + (out_1 - out_2) / 2\n",
    "\n",
    "    \"\"\"The sum_poly used in __FM_as_QUBO above is inefficient in terms of computation speed and \n",
    "    memory. In the case of FM, where the interaction terms of the decision variables are generally \n",
    "    nonzero, the following implementation using BinaryMatrix is more efficient. Here, the quadratic \n",
    "    terms in BinaryMatrix correspond to the non-diagonal terms represented by the upper triangular \n",
    "    matrix, so x(1/2) for the quadratic terms in the FM formula is unnecessary. Also, although x is \n",
    "    taken as an argument just to match the function signature with __FM_as_QUBO above (implementation \n",
    "    using sum_poly), it is not needed in this implementation using BinaryMatrix.\n",
    "    def __FM_as_QUBO(self, x, w0, w, v):\n",
    "        out_1_matrix = v @ v.T\n",
    "        out_2_matrix = np.diag((v * v).sum(axis=1))\n",
    "        matrix = BinaryMatrix(out_1_matrix - out_2_matrix + np.diag(w))\n",
    "        # Do not forget to put the constant term w0 in the second argument of BinaryQuadraticModel.\n",
    "        model = BinaryQuadraticModel(matrix, w0)\n",
    "        return model\n",
    "    \"\"\"\n",
    "\n",
    "    # A function to plot the history of i-th objective function evaluations performed within the initial training data construction (blue) and during FMQA cycles (red).\n",
    "    def plot_history(self):\n",
    "        assert self.y is not None\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        plt.plot(\n",
    "            [i for i in range(self.N0)],\n",
    "            self.y[: self.N0],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"b\",\n",
    "        )  # Objective function evaluation values at the time of initial training data generation (random process)\n",
    "        plt.plot(\n",
    "            [i for i in range(self.N0, self.N)],\n",
    "            self.y[self.N0 :],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"r\",\n",
    "        )  # Objective function evaluation values during the FMQA cycles (FMQA cycle process)\n",
    "        plt.xlabel(\"i-th evaluation of f(x)\", fontsize=18)\n",
    "        plt.ylabel(\"f(x)\", fontsize=18)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3\\. FMQA execution example\n",
    "\n",
    "<a id=\"3_1\"></a>\n",
    "### 3.1\\. Material search for the highest critical temperature\n",
    "\n",
    "Now we will perform a material search using the implemented FMQA and model functions. Since we are maximizing the critical temperature estimated from the model, we implement the objective function to return the negative value of the critical temperature and perform the FMQA to minimize this value.\n",
    "\n",
    "In the following, $N = 100$ and $N_0=60$. Thus, in the example below, $N-N_0=40$ cycles of FMQA (machine learning, search for the optimal solution with the quantum annealing or Ising machines, and evaluation of the objective function) are performed. Note that with this setup, it will take approximately 5-10 minutes to complete all FMQA cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random seed values\n",
    "seed_everything()\n",
    "\n",
    "# Size of input values (the number of pseudo-materials)\n",
    "D = 100\n",
    "\n",
    "# Property tables\n",
    "debye_temperature_table, state_density_table, interaction_table = set_properties(D)\n",
    "\n",
    "\n",
    "# Objective Function. We implement the objective function so that it returns a negative value of the critical temperature, and FMQA optimizes the choice of material to minimize this value\n",
    "\n",
    "\n",
    "def true_func(x):\n",
    "    if np.sum(x) == 0:\n",
    "        return 0\n",
    "    return -supercon_temperature(\n",
    "        x, debye_temperature_table, state_density_table, interaction_table\n",
    "    )\n",
    "\n",
    "\n",
    "N = 100  # Number of times the function can be evaluated\n",
    "N0 = 60  # Number of samples of initial training data\n",
    "k = 20  # Dimension of the vector in FM (hyperparameters)\n",
    "\n",
    "# client: Amplify client created earlier\n",
    "solver = Solver(client)\n",
    "# Generate initial training data\n",
    "X, y = gen_training_data(D, N0, true_func)\n",
    "\n",
    "# Instantiate FMQA class\n",
    "fmqa_solver = FMQA(D, N, N0, k, true_func, solver)\n",
    "# Run FMQA cycle\n",
    "pred_x = fmqa_solver.cycle(X, y)\n",
    "# Output optimization results\n",
    "print(\"pred x:\", pred_x)\n",
    "print(\"pred value:\", true_func(pred_x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2\"></a>\n",
    "### 3.2\\. Transition of objective function values during the FMQA optimization process\n",
    "\n",
    "Plotted below are the $N_0$ objective function values obtained for randomly generated input values during initial training data generation and the evolution of objective function values during the FMQA optimization process for $N-N_0$ cycles.\n",
    "\n",
    "The blue and red lines, respectively, show how the smallest objective function value is successively updated from the input values obtained by the FMQA optimization cycle (red line).\n",
    "\n",
    "In general, due to the principle of the heuristics algorithm employed in `FixstarsClient`, the solutions obtained are not reproducible, but for the material choices obtained with the parameters $N_0=60$ and $N-N_0=40$ in the sample code, the resulting critical temperature is approximately 50 K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fmqa_solver.plot_history()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_3\"></a>\n",
    "### 3.3\\. Example output from the FMQA sample program\n",
    "\n",
    "In general, due to the principle of the heuristics algorithm employed in `FixstarsClient`, the solutions obtained are not completely reproducible, but typical standard output and image output obtained when this sample code is used are shown below.\n",
    "\n",
    "- The following is a typical output obtained when this sample code is executed. When the FMQA code in [Material search for the highest critical temperature](#3_1)\" is executed under the given conditions, the following standard output is sequentially output as the FMQA cycle progresses.\n",
    "\n",
    "    ```shell\n",
    "    Generating 0-th training data set.\n",
    "    Generating 10-th training data set.\n",
    "    Generating 20-th training data set.\n",
    "    Generating 30-th training data set.\n",
    "    Generating 40-th training data set.\n",
    "    Generating 50-th training data set.\n",
    "    Starting FMQA cycles...\n",
    "    FMQA Cycle #0 variable updated, pred_y=-18.98476017536205\n",
    "    FMQA Cycle #1 \n",
    "    FMQA Cycle #2 variable updated, pred_y=-25.897204545387414\n",
    "    FMQA Cycle #3 variable updated, pred_y=-30.641568733824826\n",
    "    FMQA Cycle #4 \n",
    "    FMQA Cycle #5 variable updated, pred_y=-33.23380829087865\n",
    "    FMQA Cycle #6 \n",
    "    FMQA Cycle #7 \n",
    "    FMQA Cycle #8 variable updated, pred_y=-40.97929639761995\n",
    "    FMQA Cycle #9 \n",
    "    FMQA Cycle #10 \n",
    "    FMQA Cycle #11 \n",
    "    FMQA Cycle #12 \n",
    "    FMQA Cycle #13 \n",
    "    FMQA Cycle #14 \n",
    "    FMQA Cycle #15 \n",
    "    FMQA Cycle #16 \n",
    "    FMQA Cycle #17 \n",
    "    FMQA Cycle #18 variable updated, pred_y=-42.00895340350797\n",
    "    FMQA Cycle #19 variable updated, pred_y=-47.787495086366945\n",
    "    FMQA Cycle #20 \n",
    "    FMQA Cycle #21 variable updated, pred_y=-52.41427395241357\n",
    "    FMQA Cycle #22 \n",
    "    FMQA Cycle #23 \n",
    "    FMQA Cycle #24 \n",
    "    FMQA Cycle #25 \n",
    "    FMQA Cycle #26 \n",
    "    FMQA Cycle #27 \n",
    "    FMQA Cycle #28 \n",
    "    FMQA Cycle #29 \n",
    "    FMQA Cycle #30 \n",
    "    FMQA Cycle #31 \n",
    "    FMQA Cycle #32 \n",
    "    FMQA Cycle #33 \n",
    "    FMQA Cycle #34 \n",
    "    FMQA Cycle #35 \n",
    "    FMQA Cycle #36 \n",
    "    FMQA Cycle #37 \n",
    "    FMQA Cycle #38 variable updated, pred_y=-55.425491086604936\n",
    "    FMQA Cycle #39 \n",
    "    pred x: [0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1.\n",
    "    1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
    "    2. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
    "    3. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.\n",
    "    4. 0. 1. 1.]\n",
    "    pred value: -55.425491086604936\n",
    "    ```\n",
    "\n",
    "- The output image from `fmqa_reactor.plot_history()` as described in \"[3.2\\. Transition of objective function values during FMQA optimization process](#3_2)\" is as follows:\n",
    "\n",
    "  ![history](../figures/fmqa_1_supercon_history.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "### Exercises\n",
    "\n",
    "<a id=\"4_1\"></a>\n",
    "#### Exercise 1\n",
    "\n",
    "Using the random search approach, how many attempts will it take to find a combination of materials that achieves the same level of critical temperature as that obtained by the black box optimization? You can check this by varying the `n_cycle` in section [1.3](#1_3).\n",
    "\n",
    "- **Supplemental information**\n",
    "\n",
    "  If $\\tau_{ML}$ is the time required for one machine learning for FM, $\\tau_{QA}$ is the time required to find the optimal solution, and $\\tau_{eval}$ is the time required to evaluate the objective function, in general, the total time cost $c_t$ for the search can be described by:\n",
    "\n",
    "  $$\n",
    "  c_t = N_0 \\cdot \\tau_{eval} + (N - N_0) \\cdot (\\tau_{ML} + \\tau_{QA} + \\tau_{eval} ).\n",
    "  $$\n",
    "\n",
    "  In this example code, $\\tau_{eval}$ is relatively small because the model is used to evaluate the critical temperature. However, in general, for tasks that require black-box optimization, $\\tau_{eval} \\gg \\tau_{ML}$ and $\\tau_{eval} \\gg _{QA}$. In that case, the total time cost is:\n",
    "\n",
    "  $$\n",
    "  c_t \\sim N \\cdot \\tau_{eval}.\n",
    "  $$\n",
    "\n",
    "  In the present case, for example, if one hour is needed for each material synthesis + critical temperature measurement, and if this is done independently 24/7, approximately 4 days are required for a search for $N=100$, and 1 year for $N=10000$. Therefore, to keep the optimization cost small, keeping the number of evaluations of the objective function $N$ small is a priority.\n",
    "  \n",
    "  It will be shown that random searches generally do not approach or exceed the optimal solution by FMQA unless unrealistically-large $N$ is used (i.e., $c_t$ is enormous)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4_2\"></a>\n",
    "#### Exercise 2\n",
    "\n",
    "Let us vary the hyperparameters related to the machine learning of FM. How will the optimization accuracy and computation time change?\n",
    "\n",
    "- Hint: try to change the parameters in the model call in the `step()` function of the `FMQA` class excerpted below. (e.g., change the number of epochs `epoch` to 1/10 of the original value)\n",
    "  ```python\n",
    "    model = train(\n",
    "        X,\n",
    "        y,\n",
    "        model_class=TorchFM,\n",
    "        model_params={\"d\": self.D, \"k\": self.k},\n",
    "        batch_size=8,\n",
    "        epochs=2000,\n",
    "        criterion=nn.MSELoss(),\n",
    "        optimizer_class=torch.optim.AdamW,\n",
    "        opt_params={\"lr\": 1},\n",
    "    )\n",
    "  ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4_3\"></a>\n",
    "#### Exercise 3\n",
    "\n",
    "Think of a business or a problem in your immediate environment where black-box optimization can be utilized. What are the decision variables (input values) and the objective function? How would the objective function be evaluated?\n",
    "\n",
    "> Example: Optimization of fuel blends (hydrogen, natural gas, syngas, ammonia, steam, recirculated and flue gases, etc.) and thermochemical conditions in a next-generation gas turbine power plant. Reduce fuel procurement costs and pollutant generation while ensuring power output to meet daily demand. The objective function is the amount of pollutant generation, fuel cost, and $($electricity demand$-$power output$)^2$, and its evaluation is based on a (linear) calculation of fuel cost and full-scale simulation or measurement on an actual plant.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
