{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-Box Optimization of Model Superconducting Materials\n",
    "\n",
    "\n",
    "To illustrate the effective use of black-box optimization, this sample code describes the optimization for superconducting materials composed of pseudo-materials as an example problem. Although the present sample code performs material searches based on nonlinear algebraic models, you can perform black-box optimization with the same steps based on high-precision simulations or experimental measurement results instead of model algebraic expressions. Even in such cases, you can use this example almost as is.\n",
    "\n",
    "For a basic introduction to black-box optimization and FMQA, see \"[Black-Box Optimization with Quantum Annealing and Ising Machines](https://amplify.fixstars.com/en/demo/fmqa_0_algebra)\".\n",
    "\n",
    "Also, you can find [example programs](https://amplify.fixstars.com/en/demo#blackbox) of FMQA applications in various fields such as material search, fluid engineering, chemical plant and urban transportation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setting\n",
    "\n",
    "### Search scenario for superconducting materials\n",
    "\n",
    "Superconductivity technology is expected to be utilized in the fields of transportation, such as maglev trains, metrology, and energy. Various superconducting materials are currently being developed to realize superconductivity.\n",
    "\n",
    "The temperature at which superconductivity is achieved (critical temperature) is generally around the absolute temperature of 0 K (Kelvin) for currently confirmed superconducting materials. Because of this, superconductivity requires costly cooling to be exploited, and its application in the real world is currently limited. Therefore, the search for high-temperature superconductors is a pressing issue.\n",
    "\n",
    "Typically, the search for materials that realize superconductivity involves a trial-and-error process of selecting and synthesizing several materials, repeatedly evaluating the critical temperature of the synthesized materials by measurement, and identifying the material to be synthesized that achieves a higher critical temperature. This process of synthesis and critical temperature evaluation is considered time-consuming. For this search, a black box optimization method is used to find a combination of materials close to the optimal solution with a relatively small number of evaluations.\n",
    "\n",
    "In this example, the search for superconducting materials consisting of pseudo materials is treated as an example to illustrate the material search by a black-box optimization method (FMQA), and a critical temperature model is used to evaluate the critical temperature. Note that the critical temperature models presented below and the combinations of materials obtained are not necessarily physically accurate, and thus the example serves for illustration purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver client configuration\n",
    "\n",
    "First, we will configure the solver client of the solver we will use during the optimization cycles. In this example program, we will use Amplify AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import FixstarsClient\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set a solver client\n",
    "client = FixstarsClient()\n",
    "# If you use Amplify in a local environment, enter the Amplify API token.\n",
    "# client.token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "# Set timeout to be 2 seconds\n",
    "client.parameters.timeout = timedelta(milliseconds=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of black-box function\n",
    "\n",
    "This example code selects a combination of several materials from $D$ types of materials and performs an optimization to maximize the critical temperature of the superconducting material produced by their synthesis.\n",
    "\n",
    "In general, the critical temperature can be evaluated by experimental measurement, which requires a relatively large cost (time and money) each time the evaluation is performed. In this example code, instead of measuring the critical temperature, the following critical temperature model `supercon_temperature()` is used for evaluation. However, this function is only a substitute for experimental measurement, and its contents and parameters are treated as unknown, and the number of calls to `supercon_temperature()` is also treated as limited.\n",
    "\n",
    "The following `make_blackbox_func` function creates and returns a blackbox function `blackbox`, which is also the objective function in the present FMQA. The blackbox function `blackbox` executes `supercon_temperature()` and returns the negative value of the obtained critical temperature.\n",
    "\n",
    "In this example, optimization will progress to minimize the negative value of the obtained critical temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from typing import Callable, Any\n",
    "\n",
    "# Set the random seed\n",
    "seed = 1234\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def make_blackbox_func(d: int) -> Callable[[np.ndarray], float]:\n",
    "    \"\"\"Returns a function that takes a binary vector with size d as input and returns a float value\"\"\"\n",
    "\n",
    "    def set_properties(size: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Returns randomly chosen material properties\"\"\"\n",
    "        mu, sigma, ratio = 0.0, 1.0, 0.2\n",
    "        table1 = rng.random(size) * 1e5 * (0.1 * math.log(size) - 0.23)\n",
    "        table2 = rng.lognormal(mu, sigma, size) * ratio\n",
    "        table3 = rng.lognormal(mu, sigma, size) * ratio\n",
    "        return table1, table2, table3\n",
    "\n",
    "    def supercon_temperature(\n",
    "        x: np.ndarray,\n",
    "        debye_table: np.ndarray,\n",
    "        state_table: np.ndarray,\n",
    "        interaction_table: np.ndarray,\n",
    "    ) -> float:\n",
    "        \"\"\"For a given material choice (a binary variable vector of size d), compute and return the critical temperature. (substitute of simulation or experiment)\"\"\"\n",
    "        debye_temperature = np.sum(x * debye_table) / np.sum(x)\n",
    "        state_density = np.sum(x * state_table) / np.sum(x)\n",
    "        interaction = np.sum(x * interaction_table) / np.sum(x)\n",
    "        crit_temp = debye_temperature * math.exp(-1.0 / state_density / interaction)\n",
    "        return crit_temp\n",
    "\n",
    "    # Prepare property tables\n",
    "    debye_temperature_table, state_density_table, interaction_table = set_properties(d)\n",
    "\n",
    "    # Definition of the black-box function\n",
    "    def blackbox(x: np.ndarray) -> float:\n",
    "        \"\"\"For a given material choice (a binary vector of size d), returns the negative of the critical temperature.\"\"\"\n",
    "        assert x.shape == (d,)  # x is the 1D binary vector of size d\n",
    "        t_c = supercon_temperature(\n",
    "            x, debye_temperature_table, state_density_table, interaction_table\n",
    "        )\n",
    "        return -t_c\n",
    "\n",
    "    return blackbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the black-box function `blackbox(x)` for the critical temperature defined above is used to evaluate the critical temperature of superconducting materials synthesized from a random selection of materials. Here, `num_materials` is the number of materials to be selected, and the input `x` is a vector of size `num_materials` consisting of 0 or 1.\n",
    "\n",
    "For example, in the case of selecting the first and last of five materials to be combined, the input vector would be `x = [1, 0, 0, 0, 0, 1]`. In this case, there are $2^5-1=31$ possible choices (combinations).\n",
    "\n",
    "For `num_materials = 100`, the number of combinations is approximately $10^{30}$, and the full-search method is considered difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_materials = (\n",
    "    100  # The size of the binary decision variable (number of materials to choose from)\n",
    ")\n",
    "\n",
    "\n",
    "blackbox_func = make_blackbox_func(num_materials)\n",
    "\n",
    "# Evaluate the black-box function with the random inputs for n_cycle times, and print the minimum and average values of the blackbox function\n",
    "n_cycle = 100\n",
    "obj_min = 0.0  # The variable to save the minimum of the negative critical temperature\n",
    "obj_mean = 0.0  # The variable to save the average of the negative critical temperature\n",
    "for i in range(n_cycle):\n",
    "    x = rng.integers(0, 2, num_materials)\n",
    "    if np.sum(x) == 0:\n",
    "        continue\n",
    "    obj = blackbox_func(x)\n",
    "    if obj_min > obj:\n",
    "        obj_min = obj\n",
    "    obj_mean += obj\n",
    "obj_mean /= n_cycle\n",
    "\n",
    "print(f\"Minimum objective function value: {obj_min:.2f} K\")\n",
    "print(f\"Mean objective function value: {obj_mean:.2f} K\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMQA program implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training by machine learning\n",
    "\n",
    "This section implements the part of FMQA that learns the optimal parameters (weights and bias) of the model by machine learning. First, the `TorchFM` class representing the model by the Factorization Machine is defined using PyTorch.\n",
    "\n",
    "The following equation represents the Factorization Machine.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  f(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= \n",
    "  \\underset{\\color{red}{\\mathtt{out\\_linear}}}{\\underline{ w_0 + \\sum_{i=1}^d w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_quadratic}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^d v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^d v_{i f}^2 x_i^2 }} \\right] }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The input $x$ of this model is a vector of the length $d$ as the input to the black-box function, with the following three parameters.\n",
    "\n",
    "* $v$: 2-dimensional array of $d\\times k$.\n",
    "* $w$: 1D vector of length $d$.\n",
    "* $w_0$: scalar\n",
    "\n",
    "The only hyperparameter is $k$, which is given as a positive integer less than or equal to $d$.\n",
    "\n",
    "\n",
    "The `TorchFM` class defined below inherits from `torch.nn.Module` and is constructed from an input vector $x$ of size $d$ and a hyperparameter $k$. The hyperparameter $k$ controls the number of parameters in the model; the larger the hyperparameter, the more parameters are, the more accurate the model becomes, but also the more prone to over-fitting.\n",
    "\n",
    "The `TorchFM` class has attributes $v$, $w$, and $w_0$ of the model parameters and updates these parameters as the training proceeds. According to the above formula, the `forward` method also outputs an estimate of $y$ from the input $x$. Since the parameters $v$, $w$, and $w_0$ are needed to construct a QUBO model later, we also define a function `get_parameters` to output these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set a random seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int):\n",
    "        \"\"\"Define a model\n",
    "\n",
    "        Args:\n",
    "            d (int): Size of input vector\n",
    "            k (int): Hyperparameter k\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.v = torch.randn((d, k), requires_grad=True)\n",
    "        self.w = torch.randn((d,), requires_grad=True)\n",
    "        self.w0 = torch.randn((), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Take x as input and returns a predicted value y\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 2D tensor of (number of samples Ã— d)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 1D tensor of predicted y (size is the number of samples)\n",
    "        \"\"\"\n",
    "        out_linear = torch.matmul(x, self.w) + self.w0\n",
    "\n",
    "        out_1 = torch.matmul(x, self.v).pow(2).sum(1)\n",
    "        out_2 = torch.matmul(x.pow(2), self.v.pow(2)).sum(1)\n",
    "        out_quadratic = 0.5 * (out_1 - out_2)\n",
    "\n",
    "        out = out_linear + out_quadratic\n",
    "        return out\n",
    "\n",
    "    def get_parameters(self) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"Returns the parameters (weights and bias) v, w, w0\"\"\"\n",
    "        np_v = self.v.detach().numpy().copy()\n",
    "        np_w = self.w.detach().numpy().copy()\n",
    "        np_w0 = self.w0.detach().numpy().copy()\n",
    "        return np_v, np_w, float(np_w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function `train()` to perform machine learning of the model based on the `TorchFM` class. The input is the training data $x, y$ and an instance of the `TorchFM` model. By calling the `train()` function, the `TorchFM` parameters are trained and optimized.\n",
    "\n",
    "In general machine learning, we split the data into training and validation data, optimize the parameters using the training data, and validate the model during training using the validation data. The model is validated at each epoch, and the parameters at the epoch with the highest prediction accuracy for the validation data are saved and used as the model after learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "import copy\n",
    "\n",
    "\n",
    "def train(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model: TorchFM,\n",
    ") -> None:\n",
    "    \"\"\"Perform training of a FM model\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Training data (input vectors)\n",
    "        y (np.ndarray): Training data (output values)\n",
    "        model (TorchFM): a TorchFM model\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of iterations\n",
    "    epochs = 2000\n",
    "    # Model optimizer\n",
    "    optimizer = torch.optim.AdamW([model.v, model.w, model.w0], lr=0.1)\n",
    "    # Loss function\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    # Prepare the dataset\n",
    "    x_tensor, y_tensor = (\n",
    "        torch.from_numpy(x).float(),\n",
    "        torch.from_numpy(y).float(),\n",
    "    )\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    train_set, valid_set = random_split(dataset, [0.8, 0.2])\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Execute the training\n",
    "    min_loss = 1e18  # Save the minimum loss value\n",
    "    best_state = model.state_dict()  # Save the best model parameters\n",
    "\n",
    "    # Visualize progress using `tqdm` instead of `range`\n",
    "    for _ in trange(epochs, leave=False):\n",
    "        # Training phase\n",
    "        for x_train, y_train in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train)\n",
    "            loss = loss_func(pred_y, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            for x_valid, y_valid in valid_loader:\n",
    "                out_valid = model(x_valid)\n",
    "                loss += loss_func(out_valid, y_valid)\n",
    "            if loss < min_loss:\n",
    "                # Save the model parameter is the loss function value is updated\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                min_loss = loss\n",
    "\n",
    "    # Update model based on the trainded parameters\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the model with Amplify\n",
    "\n",
    "Next, we implement the `anneal` function to minimize the inferred machine learning model, corresponding to the lower left part of the FMQA cycle diagram (reproduced below), where the input is the model `TorchFM` class after training, and the output is a vector $x$ that minimizes the model.\n",
    "\n",
    "![](../figures/fmqa_0_algebra/fmqa_cycle.drawio.svg)\n",
    "\n",
    "Solve the following optimization problem for the class `TorchFM` model learned earlier to find the input $x$ such that the inferred model is minimized.\n",
    "\n",
    "$$\n",
    "  \\underset{x}{\\mathrm{argmin}} \\quad \n",
    "  \\underset{\\color{red}{\\mathtt{out\\_linear}}}{\\underline{ w_0 + \\sum_{i=1}^d w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_quadratic}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^d v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^d v_{i f}^2 x_i^2 }} \\right] }}\n",
    "$$\n",
    "\n",
    "The decision variable in this optimization problem is $x$. It is a one-dimensional binary variable vector of length $d$, just like the input vector to the black-box function. Also, $v$, $w$, and $w_0$, parameters in the learning phase, are constants here.\n",
    "\n",
    "We define the `anneal` function that performs the optimization using Amplify AE for the given model as follows. The `anneal` function uses the `VariableGenerator` to create a 1D binary vector `x` of length $d$, and then uses the binary variable array $x$ and $v$, $w$, and $w_0$ obtained from the `TorchFM` class to create a Factorization Machine The objective function to be optimized is created according to the formula of `Factorization Machine`.\n",
    "\n",
    "\n",
    "After building the optimization model, we will perform minimization of the objective function using the created QUBO model and the solver client `FixstarsClient` defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import VariableGenerator, Model, solve, Poly\n",
    "\n",
    "\n",
    "def anneal(torch_model: TorchFM) -> np.ndarray:\n",
    "    \"\"\"Take the parameters of the FM model and find x that gives the minimum value of the FM model as the QUBO model described by those parameters.\"\"\"\n",
    "\n",
    "    # Generate a binary variable array of size d\n",
    "    gen = VariableGenerator()\n",
    "    x = gen.array(\"Binary\", torch_model.d)\n",
    "\n",
    "    # Obtain FM parameters v, w, w0 from TorchFM\n",
    "    v, w, w0 = torch_model.get_parameters()\n",
    "\n",
    "    # Create the objective function\n",
    "    out_linear = w0 + (x * w).sum()\n",
    "    out_1 = ((x[:, np.newaxis] * v).sum(axis=0) ** 2).sum()  # type: ignore\n",
    "    out_2 = ((x[:, np.newaxis] * v) ** 2).sum()\n",
    "    objective: Poly = out_linear + (out_1 - out_2) / 2\n",
    "\n",
    "    # Construct combinatorial optimization model\n",
    "    amplify_model = Model(objective)\n",
    "\n",
    "    # Execute the optimization (pass the constructed QUBO model and the solver client created in the beginning)\n",
    "    result = solve(amplify_model, client)\n",
    "    if len(result.solutions) == 0:\n",
    "        raise RuntimeError(\"No solution was found.\")\n",
    "\n",
    "    # Return the input that minimizes the model\n",
    "    return x.evaluate(result.best.values).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the `train` function for machine learning and the `anneal` function for optimization, which are the core of FMQA, we can execute FMQA using these functions.\n",
    "\n",
    "The black-box function (corresponding to experiment or simulation) to be used for th epresent black-box optimization is the `blackbox_func()` already defined above. This function takes a NumPy one-dimensional binary vector of length $d = 100$ consisting of $0$ or $1$ and returns the negative value of the critical temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating initial training data\n",
    "\n",
    "Next, the `black-box` function $y = f(\\boldsymbol{x})$ is evaluated on the input vector $\\boldsymbol{x}$ to create $N_0$ initial training data. The `black-box` function corresponds to the results of a simulation or experiment, so you can create these data based on previous experiments or simulations.\n",
    "\n",
    "We define the `init_training_data` function to create the initial training data using a random $N_0$ input vector $x$ as follows. This function takes the `black-box` function and the number of initial training data $N_0$ and returns $N_0$ input vectors $\\boldsymbol{x}$ and corresponding output $y$ as initial training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training_data(d: int, n0: int):\n",
    "    \"\"\"Craete initial training data of n0 samples\"\"\"\n",
    "    assert n0 < 2**d\n",
    "\n",
    "    # Generate n0 random inputs of size d\n",
    "    x = rng.choice(np.array([0, 1]), size=(n0, d))\n",
    "\n",
    "    # Check if these random inputs are unique. If not, modify corresponding inputs\n",
    "    x = np.unique(x, axis=0)\n",
    "    while x.shape[0] != n0:\n",
    "        x = np.vstack((x, np.random.randint(0, 2, size=(n0 - x.shape[0], d))))\n",
    "        x = np.unique(x, axis=0)\n",
    "\n",
    "    # Evaluate black-bok function to obtain n0 output values\n",
    "    y = np.zeros(n0)\n",
    "    for i in range(n0):\n",
    "        y[i] = blackbox_func(x[i])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "N0 = 10  # Number of samples in the initlal training data\n",
    "x_init, y_init = init_training_data(num_materials, N0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have $N_0$-pair of initial training data (10 pairs of the input vector of size 100 and the output value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an FMQA cycle\n",
    "\n",
    "Using the $N_0$ pairs of initial training data created above, we will run the FMQA cycles, by performing the following operations per FMQA cycle.\n",
    "\n",
    "\n",
    "1. Train the model\n",
    "   * Construct a model of class `TorchFM` and train it by calling the `train` function on the model with initial training data `x = x_init`, `y = y_init`.\n",
    "2. Minimize the model\n",
    "   * Minimize the model by passing a trained model of class `TorchFM` to the `anneal` function to get $\\hat{x}$ that minimizes the model.\n",
    "   * If $\\hat{x}$ is already included in the training data `x`, change part of $\\hat{x}$ to avoid duplicate training data.\n",
    "3. Evaluation of black-box function\n",
    "   * Input $\\hat{x}$ to the black-box function and get output $\\hat{y}$.\n",
    "   * Add $\\hat{x}$ and $\\hat{y}$ to the traiing data `x` and `y`, respectively.\n",
    "\n",
    "An example implementation of the above is as follows: Since the execution takes approximately 10 minutes of computation time, an example output is shown in [Execution example](#execution-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of FMQA cycles\n",
    "N = 40\n",
    "\n",
    "# Initialize training data\n",
    "x, y = x_init, y_init\n",
    "\n",
    "# Perform N FMQA cycles\n",
    "# Use `tqdm` instead of `range` to visualize the progress\n",
    "for i in trange(N):\n",
    "    # Create machine-learning model\n",
    "    model = TorchFM(num_materials, k=10)\n",
    "\n",
    "    # Train the machine-learning model\n",
    "    train(x, y, model)\n",
    "\n",
    "    # Obtain the input that yields the minimum value of the trained model\n",
    "    x_hat = anneal(model)\n",
    "\n",
    "    # Ensure the uniqueness of x_hat\n",
    "    while (x_hat == x).all(axis=1).any():\n",
    "        flip_idx = rng.choice(np.arange(num_materials))\n",
    "        x_hat[flip_idx] = 1 - x_hat[flip_idx]\n",
    "\n",
    "    # Evaluate the black-box function with the estimated x_hat\n",
    "    y_hat = blackbox_func(x_hat)\n",
    "\n",
    "    # Add the evaluation results to the training data set\n",
    "    x = np.vstack((x, x_hat))\n",
    "    y = np.append(y, y_hat)\n",
    "\n",
    "    tqdm.write(f\"FMQA cycle {i}: found y = {y_hat}; current best = {np.min(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell, `x` and `y` have input-output pairs of black-box function evaluations. The number of pairs is $N_0 + N = 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives the input that gives the minimum value of the black box function evaluation and its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx = np.argmin(y)\n",
    "print(f\"best x = {x[min_idx]}\")\n",
    "print(f\"best y = {y[min_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the evolution of the evaluated values\n",
    "\n",
    "Below are $N_0$ initial training data and the evolution of the optimized evaluation values of the black-box function over $N$ FMQA cycles. The initial training data is shown in blue, and the evaluation values obtained by the FMQA cycles are shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot()\n",
    "# Evaluation results obtained during initial training data generation\n",
    "ax.plot(\n",
    "    range(N0),\n",
    "    y[:N0],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"b\",\n",
    ")\n",
    "# Evaluation results during the FMQA cycles\n",
    "ax.plot(\n",
    "    range(N0, N0 + N),\n",
    "    y[N0:],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"r\",\n",
    ")\n",
    "ax.set_xlabel(\"number of iterations\", fontsize=18)\n",
    "ax.set_ylabel(\"f(x)\", fontsize=18)\n",
    "ax.tick_params(labelsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"execution_example\"></a>\n",
    "### Execution example\n",
    "\n",
    "The output of a typical plot obtained by running this sample code is shown below. The optimization uses a QUBO solver, so the results will vary from run to run. However, you can see that the evaluated value of the black-box function decreases with the number of FMQA cycles.\n",
    "\n",
    "\n",
    "![plot_history](../figures/fmqa_1_supercon/fmqa_1_supercon_history.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
