{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-Box Optimization with Quantum Annealing and Ising Machines\n",
    "\n",
    "This example program introduces a method of black-box optimization, which may be called Factorization Machines and Quantum Annealing (FMQA). You can find [example programs](https://amplify.fixstars.com/en/demo#blackbox) of FMQA applications in various fields such as material search, fluid engineering, chemical plant and urban transportation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to FMQA\n",
    "\n",
    "### Black-box optimization\n",
    "\n",
    "FMQA is one of the black box optimization methods. Usually, in mathematical optimization, the objective is to find a set of decision variables $\\boldsymbol{x}$ such that some objective function $y = f(\\boldsymbol{x})$ is minimized (or maximized). Here, $\\boldsymbol{x}$ is assumed to be a binary variable vector with size $d$ and each element taking the value 0 or 1.\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    \\mathrm{Minimize}&\\,\\,f(\\boldsymbol{x}) \\\\\n",
    "    \\mathrm{subject\\,\\,to\\,\\,}&\\boldsymbol{x} \\in [0,1]^d\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, if information about the objective function $f(\\boldsymbol{x})$ (functional form, gradient, submodularity, convexity, etc.) is given, efficient optimization can be performed. For example, suppose the function $f(\\boldsymbol{x})$ is known (and is quadratic in $\\boldsymbol{x}$), as in some optimization problems shown in the Amplify demo tutorial. In such a case, $f(\\boldsymbol{x})$ can be used as the objective function to perform the optimization directly as a quadratic unconstrained binary optimization (QUBO: Quadratic Unconstrained Binary Optimization) problem.\n",
    "\n",
    "On the other hand, in the case of optimization to minimize (or maximize) values obtained by simulation or experiment for physical or social phenomena, the objective function $f(\\boldsymbol{x})$ corresponds to simulation or experiment, and the function cannot be described explicitly. Mathematical optimization for such an unknown objective function $f(\\boldsymbol{x})$ is called black-box optimization.\n",
    "\n",
    "In addition, evaluating such an objective function (running simulations or experiments) is usually relatively expensive (in terms of time and money, etc). Therefore, even if the set of decision variables is finite, optimization by full search is generally difficult. Therefore, an optimization method with as few objective function evaluations as possible is required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMQA introduction\n",
    "\n",
    "FMQA is a black-box optimization method that combines machine learning and quantum annealing. The method iterates through the cycles shown in the figure below, simultaneously searching for both a good approximation by a second-order polynomial of the black-box function and the input that yields the minimum value of that polynomial.\n",
    "\n",
    "![](../figures/fmqa_0_algebra/fmqa_cycle.drawio.svg)\n",
    "\n",
    "\n",
    "First, we compute a second-order polynomial that approximates the black box function using machine learning. Next, the optimization solver finds the input $x$ that minimizes the quadratic polynomial. The $x$ obtained by the Ising machine is then input to the black box function. Suppose the second-order polynomial obtained by machine learning approximates the black-box function well enough. In that case, we can expect that $\\boldsymbol{x}$ outputs a small value when input to the black-box function. If not, we can still expect a better polynomial approximation of the black box function in the next training cycle by adding the data to the training data set and performing machine learning again.\n",
    "\n",
    "The second-order polynomial approximation of the black-box function uses a machine-learning model called the Factorization Machine (FM), consisting of the following polynomial where $d$ is a constant representing the length of the input to the black-box function, $\\boldsymbol{v}$, $\\boldsymbol{w}$, and $w_0$ are the parameters of the model, and $k$ is a hyperparameter representing the size of the parameters.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  f(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= w_0 + \\langle \\boldsymbol{w}, \\boldsymbol{x}\\rangle + \\sum_{i=1}^d \\sum_{j=i+1}^d \\langle \\boldsymbol{v}_i, \\boldsymbol{v}_j \\rangle x_i x_j \\\\\n",
    " &=w_0 + \\sum_{i=1}^d w_i x_i + \\sum_{i=1}^d \\sum_{j=i+1}^d \\sum_{f=1}^k v_{if}v_{jf}x_ix_j \\\\\n",
    " &=w_0 + \\sum_{i=1}^d w_i x_i + \\frac{1}{2}\\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^d v_{i f} x_i\\right)^2 - \\sum_{i=1}^d v_{i f}^2 x_i^2\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using the FM as a machine learning model has the following advantages.\n",
    "\n",
    "* Minimization by an optimization (QUBO) solver is possible because the model is a quadratic polynomial\n",
    "* The computational complexity of the model's inference can be parameterized\n",
    "\n",
    "The hyperparameter $k$ is a positive integer less than or equal to the length $d$ of the input to the black box function, which has the effect of adjusting the number of parameters in the FM model. When $k=d$, the model has the same degrees of freedom as the QUBO interaction term, while a smaller $k$ reduces the number of parameters and suppresses over-fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMQA procedure\n",
    "\n",
    "In FMQA, we prepare the initial training data as follows, and then repeats the inference and optimization with the machine learning model.\n",
    "\n",
    "- Preparation of initial training data ($N_0$ samples)  \n",
    "  1. Prepare $N_0$ input samples $\\{\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\cdots, \\boldsymbol{x}_{N_0}\\}$ and the corresponding $N_0$ outputs $\\{f(\\boldsymbol{x}_1 ), f(\\boldsymbol{x}_2), \\cdots, \\boldsymbol{x}_{N_0}\\}$ as initial training data. \n",
    "\n",
    "- FMQA optimization cycle ($N$ times)  \n",
    "  1. Train the FM model using the (most recent) training data and obtain the FM parameters $(\\boldsymbol{v}, \\boldsymbol{w})$.  \n",
    "  1. Estimate the input $\\hat{\\boldsymbol{x}}$ that minimizes the acquisition function $g(\\boldsymbol{x})$ by using Amplify.  \n",
    "  1. Evaluate the objective function $f(\\boldsymbol{x})$ with $\\hat{\\boldsymbol{x}}$ to obtain $\\hat{y} = f(\\hat{\\boldsymbol{x}})$. \n",
    "  1. Add $(\\hat{\\boldsymbol{x}}, \\hat{y})$ to the training data.\n",
    "\n",
    "    Repeat steps 1-4 above for $N$ times.\n",
    "\n",
    "This example program shows how to run FMQA using PyTorch and the Amplify SDK. The part of the code that minimizes the learned model uses the Fixstars Amplify Annealing Engine (Amplify AE), a GPU-based annealing machine, instead of quantum annealing (QA).\n",
    "\n",
    "The following is a example program that performs black-box optimization using FMQA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver client configuration\n",
    "\n",
    "First, we will configure the solver client of the solver we will use during the optimization cycles. In this example program, we will use Amplify AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import FixstarsClient\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set a solver client\n",
    "client = FixstarsClient()\n",
    "# If you use Amplify in a local environment, enter the Amplify API token.\n",
    "# client.token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "# Set timeout to be 2 seconds\n",
    "client.parameters.timeout = timedelta(milliseconds=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of black-box function\n",
    "\n",
    "Now we define the black box function to be used for black box optimization. A black-box function is a function whose input is a 1-dimensional array of binary variables with values of 0 or 1 and whose output is a real number. However, input arrays consisting of integers and real numbers can also be considered. Examples of such use are presented [here](https://amplify.fixstars.com/en/demo#blackbox).\n",
    "\n",
    "![](../figures/fmqa_0_algebra/blackbox_func.drawio.svg)\n",
    "\n",
    "The black-box functions may simulate physical or social phenomena or return a value obtained from experiments for practical use. These are suitable for black-box optimization because they cannot be expressed in mathematical formulas, and their properties are not obvious.\n",
    "\n",
    "However, in this tutorial, instead of simulation or experiment, we will prepare an simple algebraic function as a black box function. In the following, we define the `make_blackbox_func` function to create a \"black-box\" function that takes a NumPy 1-dimensional array with $d$ elements as input and real numbers as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Any\n",
    "\n",
    "# Set random seed\n",
    "seed = 1234\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def make_blackbox_func(d: int) -> Callable[[np.ndarray], float]:\n",
    "    \"\"\"Returns a function that takes a binary vector with size d as input and returns a float value\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Q = rng.random((d, d))\n",
    "    Q = (Q + Q.T) / 2\n",
    "    Q = Q - np.mean(Q)\n",
    "\n",
    "    def blackbox(x: np.ndarray) -> float:\n",
    "        assert x.shape == (d,)  # x is a 1D array with size d\n",
    "        return x @ Q @ x  # type: ignore\n",
    "\n",
    "    return blackbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function created here is assumed to be a quadratic function, but from now on we will estimate and minimize the function, assuming that we do not know that the function created by `make_blackbox_func` is quadratic or any other properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training by machine learning\n",
    "\n",
    "This section implements the part of FMQA that learns the optimal parameters (weights and bias) of the model by machine learning. It corresponds to the lower right part of the figure below, where the input is the training data and the output is the model.\n",
    "\n",
    "! [](.../figures/fmqa_0_algebra/fmqa_cycle.drawio.svg)\n",
    "\n",
    "First, the `TorchFM` class representing the model by the Factorization Machine is defined using PyTorch.\n",
    "\n",
    "The following equation represents the Factorization Machine.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  f(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= \n",
    "  \\underset{\\color{red}{\\mathtt{out\\_linear}}}{\\underline{ w_0 + \\sum_{i=1}^d w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_quadratic}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^d v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^d v_{i f}^2 x_i^2 }} \\right] }}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The input $x$ of this model is a vector of the length $d$ as the input to the black-box function, with the following three parameters.\n",
    "\n",
    "* $v$: 2-dimensional array of $d\\times k$.\n",
    "* $w$: 1D vector of length $d$.\n",
    "* $w_0$: scalar\n",
    "\n",
    "The only hyperparameter is $k$, which is given as a positive integer less than or equal to $d$.\n",
    "\n",
    "\n",
    "The `TorchFM` class defined below inherits from `torch.nn.Module` and is constructed from an input vector $x$ of size $d$ and a hyperparameter $k$. The hyperparameter $k$ controls the number of parameters in the model; the larger the hyperparameter, the more parameters are, the more accurate the model becomes, but also the more prone to over-fitting.\n",
    "\n",
    "The `TorchFM` class has attributes $v$, $w$, and $w_0$ of the model parameters and updates these parameters as the training proceeds. According to the above formula, the `forward` method also outputs an estimate of $y$ from the input $x$. Since the parameters $v$, $w$, and $w_0$ are needed to construct a QUBO model later, we also define a function `get_parameters` to output these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set a random seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int):\n",
    "        \"\"\"Define a model\n",
    "\n",
    "        Args:\n",
    "            d (int): Size of input vector\n",
    "            k (int): Hyperparameter k\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.v = torch.randn((d, k), requires_grad=True)\n",
    "        self.w = torch.randn((d,), requires_grad=True)\n",
    "        self.w0 = torch.randn((), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Take x as input and returns a predicted value y\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 2D tensor of (number of samples × d)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 1D tensor of predicted y (size is the number of samples)\n",
    "        \"\"\"\n",
    "        out_linear = torch.matmul(x, self.w) + self.w0\n",
    "\n",
    "        out_1 = torch.matmul(x, self.v).pow(2).sum(1)\n",
    "        out_2 = torch.matmul(x.pow(2), self.v.pow(2)).sum(1)\n",
    "        out_quadratic = 0.5 * (out_1 - out_2)\n",
    "\n",
    "        out = out_linear + out_quadratic\n",
    "        return out\n",
    "\n",
    "    def get_parameters(self) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"Returns the parameters (weights and bias) v, w, w0\"\"\"\n",
    "        np_v = self.v.detach().numpy().copy()\n",
    "        np_w = self.w.detach().numpy().copy()\n",
    "        np_w0 = self.w0.detach().numpy().copy()\n",
    "        return np_v, np_w, float(np_w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function `train()` to perform machine learning of the model based on the `TorchFM` class. The input is the training data $x, y$ and an instance of the `TorchFM` model. By calling the `train()` function, the `TorchFM` parameters are trained and optimized.\n",
    "\n",
    "In general machine learning, we split the data into training and validation data, optimize the parameters using the training data, and validate the model during training using the validation data. The model is validated at each epoch, and the parameters at the epoch with the highest prediction accuracy for the validation data are saved and used as the model after learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "import copy\n",
    "\n",
    "\n",
    "def train(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model: TorchFM,\n",
    ") -> None:\n",
    "    \"\"\"Perform training of a FM model\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Training data (input vectors)\n",
    "        y (np.ndarray): Training data (output values)\n",
    "        model (TorchFM): a TorchFM model\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of iterations\n",
    "    epochs = 2000\n",
    "    # Model optimizer\n",
    "    optimizer = torch.optim.AdamW([model.v, model.w, model.w0], lr=0.1)\n",
    "    # Loss function\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    # Prepare the dataset\n",
    "    x_tensor, y_tensor = (\n",
    "        torch.from_numpy(x).float(),\n",
    "        torch.from_numpy(y).float(),\n",
    "    )\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    train_set, valid_set = random_split(dataset, [0.8, 0.2])\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Execute the training\n",
    "    min_loss = 1e18  # Save the minimum loss value\n",
    "    best_state = model.state_dict()  # Save the best model parameters\n",
    "\n",
    "    # Visualize progress using `tqdm` instead of `range`\n",
    "    for _ in trange(epochs, leave=False):\n",
    "        # Training phase\n",
    "        for x_train, y_train in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train)\n",
    "            loss = loss_func(pred_y, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            for x_valid, y_valid in valid_loader:\n",
    "                out_valid = model(x_valid)\n",
    "                loss += loss_func(out_valid, y_valid)\n",
    "            if loss < min_loss:\n",
    "                # Save the model parameter is the loss function value is updated\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                min_loss = loss\n",
    "\n",
    "    # Update model based on the trainded parameters\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the model with Amplify\n",
    "\n",
    "Next, we implement the `anneal` function to minimize the inferred machine learning model, corresponding to the lower left part of the FMQA cycle diagram (reproduced below), where the input is the model `TorchFM` class after training, and the output is a vector $x$ that minimizes the model.\n",
    "\n",
    "![](../figures/fmqa_0_algebra/fmqa_cycle.drawio.svg)\n",
    "\n",
    "Solve the following optimization problem for the class `TorchFM` model learned earlier to find the input $x$ such that the inferred model is minimized.\n",
    "\n",
    "$$\n",
    "  \\underset{x}{\\mathrm{argmin}} \\quad \n",
    "  \\underset{\\color{red}{\\mathtt{out\\_linear}}}{\\underline{ w_0 + \\sum_{i=1}^d w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_quadratic}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^d v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^d v_{i f}^2 x_i^2 }} \\right] }}\n",
    "$$\n",
    "\n",
    "The decision variable in this optimization problem is $x$. It is a one-dimensional binary variable vector of length $d$, just like the input vector to the black-box function. Also, $v$, $w$, and $w_0$, parameters in the learning phase, are constants here.\n",
    "\n",
    "We define the `anneal` function that performs the optimization using Amplify AE for the given model as follows. The `anneal` function uses the `VariableGenerator` to create a 1D binary vector `x` of length $d$, and then uses the binary variable array $x$ and $v$, $w$, and $w_0$ obtained from the `TorchFM` class to create a Factorization Machine The objective function to be optimized is created according to the formula of `Factorization Machine`.\n",
    "\n",
    "\n",
    "After building the optimization model, we will perform minimization of the objective function using the created QUBO model and the solver client `FixstarsClient` defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import VariableGenerator, Model, solve, Poly\n",
    "\n",
    "\n",
    "def anneal(torch_model: TorchFM) -> np.ndarray:\n",
    "    \"\"\"Take the parameters of the FM model and find x that gives the minimum value of the FM model as the QUBO model described by those parameters.\"\"\"\n",
    "\n",
    "    # Generate a binary variable array of size d\n",
    "    gen = VariableGenerator()\n",
    "    x = gen.array(\"Binary\", torch_model.d)\n",
    "\n",
    "    # Obtain FM parameters v, w, w0 from TorchFM\n",
    "    v, w, w0 = torch_model.get_parameters()\n",
    "\n",
    "    # Create the objective function\n",
    "    out_linear = w0 + (x * w).sum()\n",
    "    out_1 = ((x[:, np.newaxis] * v).sum(axis=0) ** 2).sum()  # type: ignore\n",
    "    out_2 = ((x[:, np.newaxis] * v) ** 2).sum()\n",
    "    objective: Poly = out_linear + (out_1 - out_2) / 2\n",
    "\n",
    "    # Construct combinatorial optimization model\n",
    "    amplify_model = Model(objective)\n",
    "\n",
    "    # Execute the optimization (pass the constructed QUBO model and the solver client created in the beginning)\n",
    "    result = solve(amplify_model, client)\n",
    "    if len(result.solutions) == 0:\n",
    "        raise RuntimeError(\"No solution was found.\")\n",
    "\n",
    "    # Return the input that minimizes the model\n",
    "    return x.evaluate(result.best.values).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing FMQA\n",
    "\n",
    "Now that we have defined the `train` function for machine learning and the `anneal` function for optimization, which are the core of FMQA, we can execute FMQA using these functions.\n",
    "\n",
    "First, we create a `black-box` function for black-box optimization as follows. This function takes a NumPy one-dimensional vector of length $d = 100$ consisting of $0$ or $1$ and returns a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100\n",
    "blackbox = make_blackbox_func(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating initial training data\n",
    "\n",
    "Next, the `black-box` function $y = f(\\boldsymbol{x})$ is evaluated on the input vector $\\boldsymbol{x}$ to create $N_0$ initial training data. The `black-box` function corresponds to the results of a simulation or experiment, so you can create these data based on previous experiments or simulations.\n",
    "\n",
    "Since we have prepared an appropriate function as a `black-box` function for simulation, we define the `init_training_data` function to create the initial training data using a random $N_0$ input vector $x$ as follows. This function takes the `black-box` function and the number of initial training data $N_0$ and returns $N_0$ input vectors $\\boldsymbol{x}$ and corresponding output $y$ as initial training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training_data(d: int, n0: int):\n",
    "    \"\"\"Craete initial training data of n0 samples\"\"\"\n",
    "    assert n0 < 2**d\n",
    "\n",
    "    # Generate n0 random inputs of size d\n",
    "    x = rng.choice(np.array([0, 1]), size=(n0, d))\n",
    "\n",
    "    # Check if these random inputs are unique. If not, modify corresponding inputs\n",
    "    x = np.unique(x, axis=0)\n",
    "    while x.shape[0] != n0:\n",
    "        x = np.vstack((x, np.random.randint(0, 2, size=(n0 - x.shape[0], d))))\n",
    "        x = np.unique(x, axis=0)\n",
    "\n",
    "    # Evaluate black-bok function to obtain n0 output values\n",
    "    y = np.zeros(n0)\n",
    "    for i in range(n0):\n",
    "        y[i] = blackbox(x[i])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "N0 = 60  # Number of samples in the initlal training data\n",
    "x_init, y_init = init_training_data(d, N0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have $N_0$-pair of initial training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_init.shape, y_init.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an FMQA cycle\n",
    "\n",
    "Using the $N_0$ pairs of initial training data created above, we will run the FMQA cycle according to the following diagram.\n",
    "\n",
    "![](../figures/fmqa_0_algebra/fmqa_cycle.drawio.svg)\n",
    "\n",
    "We perform the following operations per FMQA cycle.\n",
    "\n",
    "\n",
    "1. Train the model\n",
    "   * Construct a model of class `TorchFM` and train it by calling the `train` function on the model with initial training data `x = x_init`, `y = y_init`.\n",
    "2. Minimize the model\n",
    "   * Minimize the model by passing a trained model of class `TorchFM` to the `anneal` function to get $\\hat{x}$ that minimizes the model.\n",
    "   * If $\\hat{x}$ is already included in the training data `x`, change part of $\\hat{x}$ to avoid duplicate training data.\n",
    "3. Evaluation of black-box function\n",
    "   * Input $\\hat{x}$ to the black-box function and get output $\\hat{y}$.\n",
    "   * Add $\\hat{x}$ and $\\hat{y}$ to the traiing data `x` and `y`, respectively.\n",
    "\n",
    "An example implementation of the above is as follows: Since the execution takes a few minutes of computation time, an example output is shown in [Execution example](#execution-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of FMQA cycles\n",
    "N = 10\n",
    "\n",
    "# Initialize training data\n",
    "x, y = x_init, y_init\n",
    "\n",
    "# Perform N FMQA cycles\n",
    "# Use `tqdm` instead of `range` to visualize the progress\n",
    "for i in trange(N):\n",
    "    # Create machine-learning model\n",
    "    model = TorchFM(d, k=10)\n",
    "\n",
    "    # Train the machine-learning model\n",
    "    train(x, y, model)\n",
    "\n",
    "    # Obtain the input that yields the minimum value of the trained model\n",
    "    x_hat = anneal(model)\n",
    "\n",
    "    # Ensure the uniqueness of x_hat\n",
    "    while (x_hat == x).all(axis=1).any():\n",
    "        flip_idx = rng.choice(np.arange(d))\n",
    "        x_hat[flip_idx] = 1 - x_hat[flip_idx]\n",
    "\n",
    "    # Evaluate the black-box function with the estimated x_hat\n",
    "    y_hat = blackbox(x_hat)\n",
    "\n",
    "    # Add the evaluation results to the training data set\n",
    "    x = np.vstack((x, x_hat))\n",
    "    y = np.append(y, y_hat)\n",
    "\n",
    "    tqdm.write(f\"FMQA cycle {i}: found y = {y_hat}; current best = {np.min(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell, `x` and `y` have input-output pairs of black-box function evaluations. The number of pairs is $N_0 + N = 70$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives the input that gives the minimum value of the black box function evaluation and its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx = np.argmin(y)\n",
    "print(f\"best x = {x[min_idx]}\")\n",
    "print(f\"best y = {y[min_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the evolution of the evaluated values\n",
    "\n",
    "Below are $N_0$ initial training data and the evolution of the optimized evaluation values of the black-box function over $N$ FMQA cycles. The initial training data is shown in blue, and the evaluation values obtained by the FMQA cycles are shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot()\n",
    "# Evaluation results obtained during initial training data generation\n",
    "ax.plot(\n",
    "    range(N0),\n",
    "    y[:N0],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"b\",\n",
    ")\n",
    "# Evaluation results during the FMQA cycles\n",
    "ax.plot(\n",
    "    range(N0, N0 + N),\n",
    "    y[N0:],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"r\",\n",
    ")\n",
    "ax.set_xlabel(\"number of iterations\", fontsize=18)\n",
    "ax.set_ylabel(\"f(x)\", fontsize=18)\n",
    "ax.tick_params(labelsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"execution_example\"></a>\n",
    "### Execution example\n",
    "\n",
    "The output of a typical plot obtained by running this sample code is shown below. The optimization uses a QUBO solver, so the results will vary from run to run. However, you can see that the evaluated value of the black-box function decreases with the number of FMQA cycles.\n",
    "\n",
    "\n",
    "![plot_history](../figures/fmqa_0_algebra/fmqa_0_algebra_plot_history.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4\\. References\n",
    "\n",
    "The present black-box optimization method that combines quantum annealing and Ising machines with machine learning is called FMQA, which has been originally proposed as FMQA in the following research.\n",
    "\n",
    "- K. Kitai, J. Guo, S. Ju, S. Tanaka, K. Tsuda, J. Shiomi, and R. Tamura,\n",
    "\"Designing metamaterials with quantum annealing and factorization machines\", \n",
    "[Physical Review Research 2, 013319 (2020)](https://doi.org/10.1103/PhysRevResearch.2.013319).\n",
    "\n",
    "In this study, the search for \"metamaterials\" is carried out using FMQA, which also have shown superior performance compared to Bayesian optimization, a conventional black-box optimization method. \n",
    "\n",
    "In the following study, the same black-box optimization method is also applied to the design of photonic crystals.\n",
    "\n",
    "- T. Inoue, Y. Seki, S. Tanaka, N. Togawa, K. Ishizaki, and S. Noda, \"Towards optimization of photonic-crystal surface-emitting lasers via quantum annealing,\" [Opt. Express  30, 43503-43512 (2022)](https://doi.org/10.1364/OE.476839). \n",
    "\n",
    "These studies suggest that this optimization method (FMQA), based on FM and combinatorial optimization, may have general applicability in black-box optimization problems in various fields. In Fixstars Amplify, there are several examples of such black-box optimization in the areas of chemical reaction, fluid dynamics, as well as material search, as follows:\n",
    "\n",
    "- [Black-Box Optimization Exploration of Model Superconducting Materials](https://amplify.fixstars.com/en/demo/fmqa_1_supercon)\n",
    "- [Black-Box Optimization of Operating Condition in a Chemical Reactor](https://amplify.fixstars.com/en/demo/fmqa_2_reactor)\n",
    "- [Black-Box Optimization of Airfoil Geometry by Fluid Flow Simulation](https://amplify.fixstars.com/en/demo/fmqa_3_aerofoil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binderhub-p27q9Rop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
