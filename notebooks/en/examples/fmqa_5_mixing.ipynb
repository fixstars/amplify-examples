{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e92088",
   "metadata": {},
   "source": [
    "# Black-Box Optimization of Design Parameters for Mixing Device\n",
    "\n",
    "Mixing devices are essential in various fields, including chemical processes and food production. While their design and operating conditions significantly impact mixing efficiency (product quality), their determination often relies on experience and trial-and-error.\n",
    "\n",
    "This tutorial introduces an approach to finding optimal conditions for five parameters involved in the device design using Black-Box Optimization (BBO) with Ising machine utilization. While the specific meanings of these parameters are not stated, they should be considered as \"design choices\" that influence the performance of the mixing device. As in actual design and control, we start from a state where \"it is unclear what impact each design parameter has on the device's performance\".\n",
    "\n",
    "In this tutorial, we will evaluate the mixing device's performance (in this case, the degree of concentration variation after mixing) for different parameter combinations using the simulator. Understanding the details of the physical model or the simulator's contents is unnecessary. The primary focus of black-box optimization is on efficiently finding the best design parameters without knowing the shape and structure of the objective function.\n",
    "\n",
    "Let's try black-box optimization by utilizing Ising machines in an engineering problem setting.\n",
    "\n",
    "For a basic knowledge of black-box optimization based on machine learning and Ising machines used in this sample code, see \"[Black Box Optimization with Quantum Annealing Ising Machines](https://amplify.fixstars.com/en/demo/fmqa_0_algebra)\". For other black-box optimization examples, see [here](https://amplify.fixstars.com/en/demo#blackbox).\n",
    "\n",
    "This sample program (optimal design parameters by black-box optimization) consists of the following:\n",
    "\n",
    "- 1\\. [Objective function overview](#obj)\n",
    "  - 1\\.1\\. [Mixing simulator overview](#sim)\n",
    "  - 1\\.2\\. [Implementing the black-box objective function](#bbfunc)\n",
    "- 2\\. [Black-box optimization implementation (integer variables)](#impl)\n",
    "  - 2\\.1\\. [Defining the decision variable class](#var)\n",
    "    - 2\\.1\\.1\\. [What is domain wall encoding?](#encoding)\n",
    "    - 2\\.1\\.2\\. [Integer decision variable class `IntegerVariable`](#integer_variable)\n",
    "  - 2\\.2\\. [FM model implementation](#fm)\n",
    "  - 2\\.3\\. [Machine learning function implementation](#train)\n",
    "  - 2\\.4\\. [Solver client setup](#client)\n",
    "  - 2\\.5\\. [Optimization using the Ising machine](#opt)\n",
    "  - 2\\.6\\. [Generating initial training data](#data)\n",
    "- 3\\. [Optimization of design parameters](#exec)\n",
    "- 4\\. [Evaluating optimization results and history](#eval)\n",
    "  - 4\\.1\\. [Plotting results](#plot)\n",
    "  - 4\\.2\\. [FMQA example run](#example)\n",
    "\n",
    "---\n",
    "\n",
    "\\*In this online demo and tutorial environment, the continuous execution time is limited to about 20 minutes. If you expect the execution time to exceed 20 minutes, for example, when trying optimization by changing conditions, please copy this sample program to your local environment before execution. In that case, download the following libraries related to the MAS traffic simulator as appropriate and save them in the following directory structure.\n",
    "\n",
    "├ [fmqa_5_mixing.ipynb](https://github.com/fixstars/amplify-examples/blob/main/notebooks/en/examples/fmqa_5_mixing.ipynb)（this program）  \n",
    "└ utils/  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├ [\\_\\_init\\_\\_.py](https://github.com/fixstars/amplify-examples/blob/main/notebooks/en/examples/utils/__init__.py) （blank file）  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└ [mixing.py](https://github.com/fixstars/amplify-examples/blob/main/notebooks/en/examples/utils/mixing.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea289fb",
   "metadata": {},
   "source": [
    "<a id=\"obj\"></a>\n",
    "\n",
    "## 1\\. Objective function overview\n",
    "\n",
    "<a id=\"sim\"></a>\n",
    "\n",
    "### 1\\.1\\. Mixing simulator overview\n",
    "\n",
    "In this tutorial, we use `MixingSimulator` as the stirring simulator. Below is a conceptual diagram of the stirrer.\n",
    "\n",
    "![](../figures/fmqa_5_mixing/mixing.png)\n",
    "\n",
    "The stirrer considered in the simulator has a total of five design parameters: `x0, x1, x2, x3, x4`. Inputting these parameters into the simulator determines the specifications of the stirrer. The initial state is set with a substance (black) added to the liquid, and stirring is simulated over a specific period based on these stirrer specifications. The result of the simulation is the degree of mixing (standard deviation of concentration).\n",
    "\n",
    "Here, due to the constraints of the stirrer, the value ranges for these parameters are as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484fee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper and lower limits for each design parameter\n",
    "bounds: dict[str, tuple[int, int]] = {\n",
    "    \"x0\": (2, 10),  # 2 <= x0 <= 10\n",
    "    \"x1\": (5, 20),  # 5 <= x1 <= 20\n",
    "    \"x2\": (0, 45),  # 0 <= x2 <= 45\n",
    "    \"x3\": (1, 5),  # 1 <= x3 <= 5\n",
    "    \"x4\": (1, 4),  # 1 <= x4 <= 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04a87a",
   "metadata": {},
   "source": [
    "Below are examples of how to use the `MixingSimulator` stirring simulator.\n",
    "\n",
    "The `simulate()` method of `MixingSimulator` executes the stirring simulation and returns the standard deviation of the concentration `c_std` in the final concentration field. If the stirring is perfectly uniform, `c_std` will be zero. Additionally, the `plot_evolution()` method displays the time-series change of the concentration field.\n",
    "\n",
    "The following simulation results show that the stirring process gradually progresses over time, and the standard deviation of the concentration distribution also decreases. Finally, a standard deviation of concentration of 0.031 is obtained, which indicates a relatively non-uniform concentration distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mixing import MixingSimulator\n",
    "\n",
    "# Use the midpoint of each design parameter value range\n",
    "x0, x1, x2, x3, x4 = tuple(int(0.5 * (v[0] + v[1])) for v in bounds.values())\n",
    "\n",
    "# Initialize the simulator with the given parameter values\n",
    "simulator = MixingSimulator(x0, x1, x2, x3, x4)\n",
    "\n",
    "# Perform stirring simulation until time 500 and obtain the standard deviation of concentration, c_std\n",
    "c_std = simulator.simulate(duration=500)\n",
    "\n",
    "# Display results\n",
    "print(f\"{c_std=:.3f}\")  # Standard deviation of concentration after stirring\n",
    "simulator.plot_evolution(\n",
    "    num_snaps=5\n",
    ")  # Plot the time evolution of the concentration distribution during the stirring process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216783c5",
   "metadata": {},
   "source": [
    "<a id=\"bbfunc\"></a>\n",
    "\n",
    "### 1\\.2\\. Implementing the black-box objective function\n",
    "\n",
    "Based on the simulator `MixingSimulator` explained above, we will implement a black-box function. The black-box function `blackbox` below takes five design parameters as arguments, executes a simulation for a specific `duration`, and returns the result (the standard deviation of the final substance concentration).\n",
    "\n",
    "The goal of this tutorial is to find a set of design parameters that minimizes the standard deviation of the concentration after mixing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182969ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox(x0: int, x1: int, x2: int, x3: int, x4: int) -> float:\n",
    "    s = MixingSimulator(x0, x1, x2, x3, x4)\n",
    "    c_std = s.simulate(duration=500)\n",
    "    s.plot_evolution()\n",
    "    print(f\"{c_std=:.3f}\")\n",
    "    return c_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bbe62",
   "metadata": {},
   "source": [
    "<a id=\"impl\"></a>\n",
    "\n",
    "## 2\\. Black-box optimization implementation (integer variables)\n",
    "\n",
    "This section describes the program implementation of FMQA, a black-box optimization method that utilizes the Ising machine.\n",
    "\n",
    "The processing flow of FMQA is described in [this tutorial](https://amplify.fixstars.com/en/demo/fmqa_0_algebra), but it is performed according to the following cycle.\n",
    "\n",
    "In black-box optimization, including FMQA, the objective function is treated as a black box, allowing it to be generally applied without modifying the program implementation itself. Therefore, it is not always necessary to understand the program implementation of black-box optimization explained here.\n",
    "\n",
    "![](../figures/fmqa_5_mixing/typical_flow_en.drawio.svg)\n",
    "\n",
    "<a id=\"var\"></a>\n",
    "\n",
    "### 2\\.1\\. Defining the decision variable class\n",
    "\n",
    "Since the Ising machine used during FMQA can directly handle only binary decision variables, appropriate encoding is necessary when considering non-binary decision variables such as integers and real numbers. In this tutorial, we consider integer decision classes with domain wall encoding and address non-binary decision variables.\n",
    "\n",
    "<a id=\"encoding\"></a>\n",
    "\n",
    "#### 2\\.1\\.1\\. What is domain wall encoding?\n",
    "\n",
    "Domain-Wall Encoding is a method for converting discrete, non-binary variables (e.g., integer variables that can take values from 0 to k) into binary variables.\n",
    "\n",
    "Assume an integer variable $x$ takes $(k+1)$ possible values in $\\{0, 1, \\dots, k\\}$.\n",
    "\n",
    "In Domain-Wall Encoding, values are represented using **$k$ binary variables** $q_1, q_2, \\dots, q_k$ as follows:\n",
    "\n",
    "- When $x = i$, the first $i$ elements of the variable sequence $\\boldsymbol{q}$ are `1`, and the rest are `0`.\n",
    "- In other words, the position where the switch from `1` to `0` occurs (the domain wall) represents the value of $x$.\n",
    "\n",
    "#### Example: When $k = 4$\n",
    "\n",
    "| Value of $x$ | Encoded bit string $\\boldsymbol{q}$ |\n",
    "| ------------ | ----------------------------------- |\n",
    "| 0            | `[0, 0, 0, 0]`                      |\n",
    "| 1            | `[1, 0, 0, 0]`                      |\n",
    "| 2            | `[1, 1, 0, 0]`                      |\n",
    "| 3            | `[1, 1, 1, 0]`                      |\n",
    "| 4            | `[1, 1, 1, 1]`                      |\n",
    "\n",
    "Thus, the representation of integer values is uniquely determined by the \"wall position (domain wall)\". The above explanation applies to integer variables with a minimum value of 0. Still, it can also be extended to integer variables with a non-zero minimum value or real variables by applying a discretization technique.\n",
    "\n",
    "<a id=\"integer_variable\"></a>\n",
    "\n",
    "#### 2\\.1\\.2\\. Integer decision variable class `IntegerVariable`\n",
    "\n",
    "The `IntegerVariable` class, to be implemented below, efficiently represents integer decision variables by internally using the Amplify SDK's binary variables and applying domain wall encoding.\n",
    "\n",
    "The core functionality of the `IntegerVariable` class is the conversion between integer values and their binary representations.\n",
    "\n",
    "- `encode(self, x: int) -> np.ndarray`:\n",
    "  This method converts the integer value `x` passed as an argument into a binary vector (NumPy array) based on the corresponding domain wall encoding. For example, encoding `x=3` for a variable with `bounds=(0, 5)` will return an array like `[1., 1., 1., 0., 0.]`.\n",
    "\n",
    "- `decode(self, x: np.ndarray) -> int`:\n",
    "  This method takes the binary variable results (NumPy array) computed by the Ising machine and decodes them back into their original integer values. Following the domain wall encoding, it reconstructs the original integer value by counting the number of ones in the array and adding the lower bound.\n",
    "\n",
    "Additionally, the `IntegerVariable` class provides the following two properties:\n",
    "\n",
    "- `constraint`:\n",
    "  This property returns the domain wall constraint considered for this integer variable. In optimization, this constraint needs to be taken into account. This ensures that the Ising machine handles the associated binary variables (below `binary_variables`) according to the domain wall encoding.\n",
    "\n",
    "- `binary_variables`:\n",
    "  This property returns the vector of Amplify SDK binary variables that constitute this integer variable. It can be used when you want to reference the integer variable directly as a combination of binary variables.\n",
    "\n",
    "The implementation below also considers `Variables`, which collectively manages multiple `IntegerVariable` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ffc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import amplify\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IntegerVariable:\n",
    "    \"\"\"Class for integer decision variables for black-box optimization. Encodes and decodes integers using domain wall encoding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, bounds: tuple[int, int], variable_generator: amplify.VariableGenerator\n",
    "    ):\n",
    "        self._bounds = bounds\n",
    "        self._q = variable_generator.array(\"Binary\", bounds[1] - bounds[0])\n",
    "        self._constraint = amplify.domain_wall(self._q[::-1])\n",
    "\n",
    "    @property\n",
    "    def constraint(self) -> amplify.Constraint:\n",
    "        \"\"\"Return the constraints required for encoding integer decision variables.\"\"\"\n",
    "        return self._constraint\n",
    "\n",
    "    @property\n",
    "    def binary_variables(self) -> amplify.PolyArray:\n",
    "        \"\"\"Return the binary variable vector of the Amplify SDK that constitutes the integer decision variable.\"\"\"\n",
    "        return self._q\n",
    "\n",
    "    def encode(self, x: int) -> np.ndarray:\n",
    "        \"\"\"Encode and binarize decision variable values.\"\"\"\n",
    "        if x < self._bounds[0] or x > self._bounds[1]:\n",
    "            raise ValueError(f\"x must be in {self._bounds}\")\n",
    "        ret = np.zeros(len(self._q))\n",
    "        ret[0 : x - self._bounds[0]] = 1\n",
    "        return ret\n",
    "\n",
    "    def decode(self, x: np.ndarray) -> int:\n",
    "        \"\"\"Decode binary values to integer decision variable values.\"\"\"\n",
    "        if x.shape != self._q.shape:\n",
    "            raise ValueError(f\"x must be of shape {self._q.shape}\")\n",
    "        return x.sum() + self._bounds[0]\n",
    "\n",
    "\n",
    "class Variables:\n",
    "    \"\"\"Class that manages a list composed of multiple integer decision variables.\"\"\"\n",
    "\n",
    "    def __init__(self, variable_list: list[IntegerVariable]):\n",
    "        self._variable_list = variable_list\n",
    "\n",
    "    def encode(self, x: list[int]) -> np.ndarray:\n",
    "        \"\"\"Encode and binarize decision variable values.\"\"\"\n",
    "        ret: list[int] = []\n",
    "        for i, var in enumerate(self._variable_list):\n",
    "            ret += var.encode(x[i]).tolist()\n",
    "        return np.array(ret)\n",
    "\n",
    "    def decode(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Decode binary values to integer decision variable values.\"\"\"\n",
    "        ret: list[int] = []\n",
    "        ista = 0\n",
    "        for var in self._variable_list:\n",
    "            iend = ista + len(var.binary_variables)\n",
    "            ret.append(var.decode(x[ista:iend]))\n",
    "            ista = iend\n",
    "        return np.array(ret, dtype=int)\n",
    "\n",
    "    @property\n",
    "    def constraints(self) -> amplify.ConstraintList:\n",
    "        \"\"\"Return the constraints required for encoding all integer decision variables.\"\"\"\n",
    "        return amplify.ConstraintList([var.constraint for var in self._variable_list])\n",
    "\n",
    "    @property\n",
    "    def binary_variables(self) -> amplify.PolyArray:\n",
    "        \"\"\"Return the binary variable vector of the Amplify SDK that constitutes all integer decision variables.\"\"\"\n",
    "        ret = np.array([])\n",
    "        for var in self._variable_list:\n",
    "            ret = np.concatenate((ret, var.binary_variables))  # type: ignore\n",
    "        return amplify.PolyArray(ret.tolist())\n",
    "\n",
    "    def __getitem__(self, i: int) -> IntegerVariable:\n",
    "        return self._variable_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c466d",
   "metadata": {},
   "source": [
    "<a id=\"fm\"></a>\n",
    "\n",
    "### 2\\.2\\. FM model implementation\n",
    "\n",
    "We will implement a `TorchFM` class using PyTorch to define the FM model, similar to how a standard machine learning model is typically constructed. The FM model is a machine learning model represented by the following polynomial. Here, $\\boldsymbol{x}$ represents the variables, $d$ is a constant representing the length of the input to the black-box function, $\\boldsymbol{v}$, $\\boldsymbol{w}$, and $w_0$ are the model coefficients (weights and biases in machine learning terms), and $k$ is a hyperparameter representing the size of the parameters.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  f(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &=\n",
    "  \\underset{\\color{red}{\\mathtt{out\\_linear}}}{\\underline{ w_0 + \\sum_{i=1}^d w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_quadratic}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^d v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^d v_{i f}^2 x_i^2 }} \\right] }}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b684b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Fix the random seed\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int):\n",
    "        \"\"\"Build the model\n",
    "\n",
    "        Args:\n",
    "            d (int): The size of the input vector\n",
    "            k (int): Parameter k\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.v = nn.Parameter(torch.randn((d, k)))\n",
    "        self.w = nn.Parameter(torch.randn((d,)))\n",
    "        self.w0 = nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes input x and outputs an estimate of y.\"\"\"\n",
    "        out_linear = torch.matmul(x, self.w) + self.w0\n",
    "        out_1 = torch.matmul(x, self.v).pow(2).sum(1)\n",
    "        out_2 = torch.matmul(x.pow(2), self.v.pow(2)).sum(1)\n",
    "        out_quadratic = 0.5 * (out_1 - out_2)\n",
    "\n",
    "        out = out_linear + out_quadratic\n",
    "        return out\n",
    "\n",
    "    def get_parameters(self) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"Output parameters v, w, and w0.\"\"\"\n",
    "        np_v = self.v.detach().numpy().copy()\n",
    "        np_w = self.w.detach().numpy().copy()\n",
    "        np_w0 = self.w0.detach().numpy().copy()\n",
    "        return np_v, np_w, float(np_w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c0388",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "\n",
    "### 2\\.3\\. Machine learning function implementation\n",
    "\n",
    "Next, we will implement the function `train` for training the `TorchFM` model defined above. This process is also handled in the same way as standard machine learning. Still, as an essential performance metric for FM models in black-box optimization, we separately display the correlation coefficient between the predicted values of the trained model and the actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62755ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "import copy\n",
    "\n",
    "\n",
    "def train(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model: TorchFM,\n",
    ") -> None:\n",
    "    \"\"\"Train the FM model.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Training data (input vectors)\n",
    "        y (np.ndarray): Training data (output values)\n",
    "        model (TorchFM): TorchFM model\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of iterations\n",
    "    epochs = 2000\n",
    "    # Model optimization function\n",
    "    # optimizer = torch.optim.AdamW([model.v, model.w, model.w0], lr=0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)  # type: ignore\n",
    "    # Loss function\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    # Prepare dataset\n",
    "    x_tensor, y_tensor = (torch.from_numpy(x).float(), torch.from_numpy(y).float())\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "    train_set, valid_set = random_split(dataset, [0.8, 0.2])\n",
    "    if len(valid_set) == 0:\n",
    "        valid_set = train_set\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    min_loss = 1e18  # Save the minimum value of the loss function\n",
    "    best_state = model.state_dict()  # Save the best model parameters of the model\n",
    "\n",
    "    # Use the `tqdm` module to display progress instead of `range`.\n",
    "    for _ in trange(epochs, leave=False):\n",
    "        # Training process\n",
    "        for x_train, y_train in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred_y = model(x_train)\n",
    "            loss = loss_func(pred_y, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation process\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            for x_valid, y_valid in valid_loader:\n",
    "                out_valid = model(x_valid)\n",
    "                loss += loss_func(out_valid, y_valid)\n",
    "            if loss < min_loss:\n",
    "                # Save the parameters when the loss function value is updated\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                min_loss = loss\n",
    "\n",
    "    # Update the model with the trained parameters\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    # Display the correlation coefficient between the predicted values of the trained FM model and the true values\n",
    "    print(\n",
    "        f\"corrcoef: {torch.corrcoef(torch.stack((model(x_tensor), y_tensor)))[0, 1].detach()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3a7d5",
   "metadata": {},
   "source": [
    "<a id=\"client\"></a>\n",
    "\n",
    "### 2\\.4\\. Solver client setup\n",
    "\n",
    "We will configure the Ising machine (solver client) for use in black-box optimization. In this tutorial, we use Fixstars Amplify Annealing Engine (Amplify AE). If running in a local environment, please enter the API token for Fixstars Amplify, which can be obtained for free.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import AmplifyAEClient\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set the solver client to Amplify AE\n",
    "client = AmplifyAEClient()\n",
    "# When running in a local environment, please uncomment and enter the Amplify AE access token\n",
    "# client.token = \"AE/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "client.parameters.time_limit_ms = timedelta(milliseconds=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90110ea",
   "metadata": {},
   "source": [
    "<a id=\"opt\"></a>\n",
    "\n",
    "### 2\\.5\\. Optimization using the Ising machine\n",
    "\n",
    "In this section, we define a function to optimize the trained FM model above. The procedure is as follows:\n",
    "\n",
    "1. Based on the model coefficients of the trained FM model, construct a QUBO model mathematically equivalent to the FM model using Amplify SDK's decision variables.\n",
    "2. Optimize the constructed QUBO model using an Ising machine.\n",
    "3. Return the optimization results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import Model, solve, Poly\n",
    "\n",
    "# Generate decision variables\n",
    "gen = amplify.VariableGenerator()\n",
    "\n",
    "var_list = [IntegerVariable(bounds=b, variable_generator=gen) for b in bounds.values()]\n",
    "variables = Variables(var_list)\n",
    "\n",
    "# The two lines above are essentially the same as the following process.\n",
    "# x0 = IntegerVariable(bounds=bounds[\"x0\"], variable_generator=gen)\n",
    "# x1 = IntegerVariable(bounds=bounds[\"x1\"], variable_generator=gen)\n",
    "# x2 = IntegerVariable(bounds=bounds[\"x2\"], variable_generator=gen)\n",
    "# x3 = IntegerVariable(bounds=bounds[\"x3\"], variable_generator=gen)\n",
    "# x4 = IntegerVariable(bounds=bounds[\"x4\"], variable_generator=gen)\n",
    "# variables = Variables([x0, x1, x2, x3, x4])\n",
    "\n",
    "# Constraints required for decision variable encoding\n",
    "constraints = variables.constraints\n",
    "\n",
    "\n",
    "def anneal(torch_model: TorchFM) -> np.ndarray:\n",
    "    \"\"\"Take the parameters of an FM model, find the x that yields the minimum value of the FM model described by those parameters.\"\"\"\n",
    "\n",
    "    # Get parameters v, w, and w0 from TorchFM\n",
    "    v, w, w0 = torch_model.get_parameters()\n",
    "\n",
    "    # Get binary decision variables of Amplify\n",
    "    x = variables.binary_variables\n",
    "\n",
    "    # Create a QUBO model (objective function) equivalent to the FM model\n",
    "    out_linear = w0 + (x * w).sum()\n",
    "    out_1 = ((x[:, np.newaxis] * v).sum(axis=0) ** 2).sum()  # type: ignore\n",
    "    out_2 = ((x[:, np.newaxis] * v) ** 2).sum()\n",
    "    objective: Poly = out_linear + (out_1 - out_2) / 2\n",
    "\n",
    "    # Make Ampify model\n",
    "    amplify_model = Model(objective, constraints)\n",
    "\n",
    "    # Minimize (pass the constructed model and the solver client)\n",
    "    result = solve(amplify_model, client)\n",
    "    if len(result.solutions) == 0:\n",
    "        raise RuntimeError(\"No solution was found.\")\n",
    "\n",
    "    # Return the input vector that minimizes the model (candidate for optimal design parameters)\n",
    "    return x.evaluate(result.best.values).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a962a4",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "\n",
    "### 2\\.6\\. Generating initial training data\n",
    "\n",
    "We evaluate the black-box function using randomly generated input vectors. The `n_0` input-output pairs obtained in this way are adopted as the initial training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_input() -> np.ndarray:\n",
    "    x: list[int] = []\n",
    "    for v_min, v_max in bounds.values():\n",
    "        x.append(rng.integers(v_min, v_max + 1))\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "def init_training_data(num_samples: int):\n",
    "    # Generate n0 input values of length d using random numbers\n",
    "    data: list[np.ndarray] = []\n",
    "    for _ in range(num_samples):\n",
    "        data.append(generate_random_input())\n",
    "    x = np.array(data)\n",
    "\n",
    "    # Remove duplicates in input values\n",
    "    x = np.unique(x, axis=0)\n",
    "    while x.shape[0] != num_samples:\n",
    "        x = np.vstack((x, generate_random_input()))\n",
    "        x = np.unique(x, axis=0)\n",
    "\n",
    "    # Evaluate the blackbox function to obtain n0 outputs corresponding to the input values\n",
    "    y = np.zeros(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        y[i] = blackbox(*x[i])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "n_0 = 10  # Number of initial training data\n",
    "x, y = init_training_data(num_samples=n_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78857ede",
   "metadata": {},
   "source": [
    "<a id=\"exec\"></a>\n",
    "\n",
    "## 3\\. Optimization of design parameters\n",
    "\n",
    "We will execute black-box optimization consisting of `n` cycles using the functions and classes implemented so far. In the code below, we set the number of times the objective function is evaluated to `n = 5`. This setting is for minimal operation confirmation in this demo/tutorial environment with a limited execution time. For execution conditions and examples of actual black-box optimization, please refer to \"[FMQA Execution Examples](#example)\".\n",
    "\n",
    "For execution, we first encode the input vectors of the initial training data and convert them into binary values. While the FM model's training data considers the binarized `x_encoded`, it evaluates the black-box function using the decoded integer decision variables `x_hat_decoded`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of FMQA cycles\n",
    "n = 5  # For minimal operation check\n",
    "\n",
    "# Encode the initial training data (x) into binary values\n",
    "x_encoded = np.array([variables.encode(x[i]) for i in range(x.shape[0])])\n",
    "\n",
    "# Iterate N times\n",
    "# Display progress using the `tqdm` module instead of `range`\n",
    "for i in trange(n):\n",
    "    # Create machine learning model\n",
    "    model = TorchFM(len(x_encoded[0]), k=10)\n",
    "\n",
    "    # Execute model training\n",
    "    train(x_encoded, y, model)\n",
    "\n",
    "    # Get the input vector (encoded in binary) that gives the minimum value of the trained model\n",
    "    x_hat = anneal(model)\n",
    "\n",
    "    # If x_hat is identical to a sample in the training data, regenerate it randomly\n",
    "    while (x_hat == x_encoded).all(axis=1).any():\n",
    "        x_hat_random = generate_random_input()\n",
    "        x_hat = variables.encode(x_hat_random.tolist())  # type: ignore\n",
    "        print(\"deduplication\")\n",
    "\n",
    "    # Decode binary decision variable values to integer decision variable values\n",
    "    x_hat_decoded = variables.decode(x_hat)\n",
    "\n",
    "    # Evaluate the black-box function using the estimated input vector\n",
    "    y_hat = blackbox(*x_hat_decoded)\n",
    "\n",
    "    # Add the evaluated value to the dataset\n",
    "    x_encoded = np.vstack((x_encoded, x_hat))\n",
    "    y = np.append(y, y_hat)\n",
    "\n",
    "    tqdm.write(f\"FMQA cycle {i}: found y = {y_hat}; current best = {np.min(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947d169",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "\n",
    "## 4\\. Evaluating optimization results and history\n",
    "\n",
    "<a id=\"plot\"></a>\n",
    "\n",
    "### 4\\.1\\. Plotting results\n",
    "\n",
    "The following functions plot the transition of objective function evaluation values during the initial learning data generation process and the optimization cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Evaluation value of the black-box function for initial teacher data generation\n",
    "ax.plot(\n",
    "    range(-n_0 + 1, 1),\n",
    "    y[:n_0],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"b\",\n",
    ")\n",
    "\n",
    "# Evaluation value of the black-box function for FMQA cycles\n",
    "ax.plot(\n",
    "    range(1, n + 1),\n",
    "    y[n_0:],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"-\",\n",
    "    color=\"r\",\n",
    ")\n",
    "\n",
    "# History of updates to the minimum value of the objective function\n",
    "ax.plot(\n",
    "    range(-n_0 + 1, n + 1),\n",
    "    [y[0]] + [min(y[:i]) for i in range(2, n_0 + n + 1)],\n",
    "    linestyle=\"--\",\n",
    "    color=\"k\",\n",
    ")\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"number of iterations\", fontsize=18)\n",
    "ax.set_ylabel(\"f(x)\", fontsize=18)\n",
    "ax.tick_params(labelsize=18)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1e-2, 2e-1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"best objective: {np.min(y):.3f}\")\n",
    "print(f\"best solution: {variables.decode(x_encoded[np.argmin(y)]).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57edd02",
   "metadata": {},
   "source": [
    "<a id=\"example\"></a>\n",
    "\n",
    "### 4\\.2\\. FMQA example run\n",
    "\n",
    "In general, due to the nature of the heuristic algorithm employed by `AmplifyAEClient`, there is no absolute reproducibility in the obtained solutions. However, we present a typical execution result obtained when running the sample code below.\n",
    "\n",
    "The figure below shows an example of execution when `n = 50`, illustrating the optimization history (the transition of black-box function values and the history of best solution updates).\n",
    "\n",
    "![](../figures/fmqa_5_mixing/output_n50_history.png)\n",
    "\n",
    "While there are increases and decreases in the objective function values in each optimization cycle, on average, the design parameters that stirr more uniformly are explored as the optimization cycles progress. In this execution example, a concentration standard deviation of 0.014 was obtained as the final best solution, and the stirring process at those design parameters is shown below.\n",
    "\n",
    "![](../figures/fmqa_5_mixing/output_n50_simulation.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
