{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量子アニーリング・イジングマシンによるブラックボックス最適化\n",
    "\n",
    "本サンプルコードでは、ブラックボックス最適化の一手法である FMQA を紹介するために、ある代数式を未知のブラックボックス関数と見なし、その出力値を最小化するような入力値を推定します。より現実的なモデルケースにおける FMQA の実施例やサンプルコードは、以下のリンクをご覧ください。\n",
    "\n",
    "- [ブラックボックス最適化によるモデル超電導材料の探索](./fmqa_1_supercon.ipynb)\n",
    "- [ブラックボックス最適化による化学プラントにおける生産量最大化](./fmqa_2_reactor.ipynb)\n",
    "- [ブラックボックス最適化と流体シミュレーションによる翼形状の最適化](./fmqa_3_aerofoil.ipynb)\n",
    "\n",
    "また、最適化対象が既知関数で2次式である場合、二次制約なし二値最適化（QUBO: Quadratic Unconstrained Binary Optimization）形式での最適化が可能です。QUBO問題の解説やサンプルコード、Amplify の使い方は、以下のリンクをご覧ください（一部抜粋）。\n",
    "\n",
    "- [はじめての Amplify](https://amplify.fixstars.com/ja/demo/1-tutorial-basic)\n",
    "- [組み合わせ最適化とは](https://amplify.fixstars.com/ja/demo/1-tutorial-combinatorial-optimization)\n",
    "- [巡回セールスマン問題](https://amplify.fixstars.com/ja/demo/tsp)\n",
    "\n",
    "\n",
    "本ノートブックは、以下の章立ての構成となっています。\n",
    "\n",
    "- 1\\. [FMQA の説明](#1)\n",
    "  - 1.1\\. [ブラックボックス最適化とは](#1_1)\n",
    "  - 1.2\\. [ベイズ最適化とは](#1_2)\n",
    "  - 1.3\\. [FMQA とは](#1_3)\n",
    "  - 1.4\\. [FMQA のフロー](#1_4)\n",
    "- 2\\. [FMQA のプログラム実装](#2)\n",
    "  - 2.1\\. [乱数の初期化](#2_1)\n",
    "  - 2.2\\. [クライアントの設定](#2_2)\n",
    "  - 2.3\\. [PyTorch による FM の実装](#2_3)\n",
    "  - 2.4\\. [初期教師データの作成](#2_4)\n",
    "  - 2.5\\. [FMQA サイクルの実行クラス](#2_5)\n",
    "- 3\\. [FMQA の実行](#3)\n",
    "  - 3.1\\. [$\\boldsymbol{x}$ の2次式に対する最適化](#3_1)\n",
    "  - 3.2\\. [FMQA 最適化過程における目標関数値の推移](#3_2)\n",
    "  - 3.3\\. [本 FMQA サンプルコード実行例](#3_3)\n",
    "  - 3.4\\. [まとめ](#3_4)\n",
    "  - 3.5\\. [参考](#3_5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1\\. FMQA の説明\n",
    "\n",
    "<a id=\"1_1\"></a>\n",
    "### 1.1\\. ブラックボックス最適化とは\n",
    "\n",
    "FMQA は、ベイズ最適化に似たブラックボックス最適化法の一つです。通常、数理最適化では、何らかの目的関数 $f(\\boldsymbol{x})$ を最小化（あるいは最大化）するような決定変数 $\\boldsymbol{x}$ を推定することを目的とします。ここで、目的関数 $f(\\boldsymbol{x})$ に関する情報（関数形、勾配、劣モジュラ性、凸性等）が与えられている場合、効率的な最適化が可能です。\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    \\mathrm{Minimize}&\\,\\,f(\\boldsymbol{x}) \\\\\n",
    "    \\mathrm{subject\\,\\,to\\,\\,}&\\boldsymbol{x} \\in [0,1]^D\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "例えば、Amplify のデモ・チュートリアルで紹介しているいくつかの最適化問題のように、$f(\\boldsymbol{x})$ の関数が既知（かつ $\\boldsymbol{x}$ の2次式）の場合、$f(\\boldsymbol{x})$ を目的関数とすることで、直接、二次制約なし二値最適化（QUBO: Quadratic Unconstrained Binary Optimization）としての最適化実施が可能です。\n",
    "\n",
    "※ここで、$\\boldsymbol{x}$ としてバイナリ変数ベクトルを仮定しますが、非バイナリ変数を one-hot エンコーディング等を使い、バイナリ変数に変換することができます。そのようなサンプルは、[ブラックボックス最適化と流体シミュレーションによる翼形状の最適化](./fmqa_3_aerofoil.ipynb)でご覧になれます。\n",
    "\n",
    "一方、物理現象や社会現象に対するシミュレーションや実験によって得られる値を最小化（または最大化）する最適化の場合、目的関数 $f(\\boldsymbol{x})$ はシミュレーションあるいは実験ということになり、目的関数を具体的な式で記述することはできません。このような未知の目的関数 $f(\\boldsymbol{x})$ に対して行う数理最適化のことをブラックボックス最適化と呼びます。また、そのような目的関数の評価（シミュレーションや実験の実施）には、一般的に比較的大きなコストが必要なため、決定変数の集合が有限であっても、全検索による最適化は困難な場合が多く、できるだけ少ない目的関数の評価回数での最適化が要求されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_2\"></a>\n",
    "### 1.2\\. ベイズ最適化とは\n",
    "\n",
    "ベイズ最適化では、以下の最適化サイクルを繰り返すことでブラックボックス最適化を実施します。\n",
    "\n",
    "1. 訓練データから獲得関数 $g(\\boldsymbol{x})$ を構築\n",
    "1. 獲得関数 $g(\\boldsymbol{x})$ が最小となる点 $\\hat{\\boldsymbol{x}}$ を推定\n",
    "1. 目的関数 $\\hat{y} = f(\\hat{\\boldsymbol{x}})$ の評価結果 $(\\hat{\\boldsymbol{x}}, \\hat{y})$を訓練データに追加\n",
    "\n",
    "このサイクルの繰り返しに伴い、最適化点近傍における獲得関数 $g(\\boldsymbol{x})$ の予測精度が向上し、その結果、得られる $\\hat{\\boldsymbol{x}}$ は、目的関数 $f(\\boldsymbol{x})$ を最小化する真の決定変数に近い値を取ることが期待されます。一方で、このベイズ最適化のサイクルにおいては、次の2つの課題があります。\n",
    "\n",
    "1. 獲得関数 $g(\\boldsymbol{x})$ の構築\n",
    "2. 獲得関数最小化を実現する $\\hat{\\boldsymbol{x}}$ の推定\n",
    "\n",
    "ベイズ最適化で重要となるこれら2つの課題を解決し、ブラックボックス最適化を実現する汎用的な手法として、次に説明する FMQA があります。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_3\"></a>\n",
    "### 1.3\\. FMQA とは\n",
    "\n",
    "ベイズ最適化で必要な獲得関数 $g(\\boldsymbol{x})$ として、次式のような機械学習モデルの一種である Factorization Machine (FM) を用いる場合を考えます。\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "  g(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= w_0 + \\langle \\boldsymbol{w}, \\boldsymbol{x}\\rangle + \\sum_{i=1}^D \\sum_{j=i+1}^D \\langle \\boldsymbol{v}_i, \\boldsymbol{v}_j \\rangle x_i x_j \\\\\n",
    " &=w_0 + \\sum_{i=1}^D w_i x_i + \\sum_{i=1}^D \\sum_{j=i+1}^D \\sum_{f=1}^k v_{if}v_{jf}x_ix_j \\\\\n",
    " &=w_0 + \\sum_{i=1}^D w_i x_i + \\frac{1}{2}\\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^D v_{i f} x_i\\right)^2 - \\sum_{i=1}^D v_{i f}^2 x_i^2\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで、FM は $\\boldsymbol{x}$ の2次式であるため、上式は QUBO による最適化が可能な関数形となります。式中の $\\boldsymbol{w}$ や $\\boldsymbol{v}$（$v_{ij}$, $w_i$）は、上式のモデルを機械学習した後に得られる FM パラメータ（機械学習における重みやバイアス）であり、$k$ はハイパーパラメータです。\n",
    "\n",
    "FM パラメータ数は、ハイパーパラメータである $k$ に依存し、$k=D$ のとき、FM は QUBO の相互作用項と同じ自由度がある一方、$k$ を小さくすることで FM パラメータ数を減らし過学習を抑制する効果があります。\n",
    "\n",
    "このように、獲得関数 $g(\\boldsymbol{x})$ に FM を採用し、その最適化を量子アニーリング（QA）やイジングマシンを用いて実施することで、ベイズ最適化における前述の課題を解決し、一般的な問題に適用することができます。このように、量子アニーリング・イジングマシンと機械学習を融合して行うブラックボックス最適化手法を FMQA と呼ぶ場合があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1_4\"></a>\n",
    "### 1.4\\. FMQA のフロー\n",
    "\n",
    "FMQA の実施手順は、上述ベイズ最適化のサイクルと同様ですが、次のように進めます。\n",
    "\n",
    "まず、目的関数評価に掛かる時間・金銭コストから、最適化において実施可能な目的関数評価の回数 $N$ を見積ります。例えば、一度の目的関数評価（実験又はシミュレーション）に1時間かかり、FMQA による最適化を1日で終える必要がある場合、最大でも $N=24$ であると考えられます。そして、$N_0<N$ であるような初期教師データのサンプル数 $N_0$ を決定し、以下の通り、初期教師データを準備、そして、FMQA サイクルを $N-N_0$ 回実施します。\n",
    "\n",
    "- 初期教師データの事前準備 ($N_0$ セット)  \n",
    "  1. 初期教師データとして、$N_0$ 個の入力サンプル $\\{\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\cdots, \\boldsymbol{x}_{N_0}\\}$ と、対応する $N_0$ 個の出力 $\\{f(\\boldsymbol{x}_1), f(\\boldsymbol{x}_2), \\cdots, \\boldsymbol{x}_{N_0}\\}$ を得る。<br><br>  \n",
    "\n",
    "\n",
    "- FMQA による最適化サイクルの実施 ($N-N_0$ 回)  \n",
    "  1.  （最新の）教師データに基づき FM を機械学習し、FM パラメータ $(\\boldsymbol{v}, \\boldsymbol{w})$ を（再度）取得する。\n",
    "  2.  獲得関数 $g(\\boldsymbol{x})$ を最小とする入力 $\\hat{\\boldsymbol{x}}$ を Amplify を用いて推定。\n",
    "  3.  目的関数 $f(\\boldsymbol{x})$ を $\\hat{\\boldsymbol{x}}$ で評価することで、$\\hat{y} = f(\\hat{\\boldsymbol{x}})$ を求める。\n",
    "  4.  教師データに $(\\hat{\\boldsymbol{x}}, \\hat{y})$ を追加する。\n",
    "   \n",
    "   上記1～4を $N-N_0$ 回繰り返す。\n",
    "\n",
    "上記 FMQA サイクルの繰り返しと共に、最適化点近傍における FM の予測精度が向上し、量子アニーリング・イジングマシンによるより良い $\\hat{\\boldsymbol{x}}$ の推定が期待されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2\\. FMQA のプログラム実装\n",
    "\n",
    "ここでは、FMQA のプログラム実装を次のように行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_1\"></a>\n",
    "### 2.1\\. 乱数の初期化\n",
    "\n",
    "実行毎に機械学習結果が変わらないようにするための、乱数seed値の初期化関数 `seed_everything()` を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_2\"></a>\n",
    "### 2.2\\. クライアントの設定\n",
    "\n",
    "Amplify のクライアントを作成し、必要なパラメータを設定します。 以下では、イジングマシンによる一度の探索時間を1秒に設定しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify.client import FixstarsClient\n",
    "\n",
    "client = FixstarsClient()\n",
    "client.parameters.timeout = 1000  # タイムアウト1秒\n",
    "# client.token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # ローカル環境等で使用する場合は、Amplify AEのアクセストークンを入力してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_3\"></a>\n",
    "### 2.3\\. PyTorch による FM の実装\n",
    "\n",
    "FM の学習と推論を PyTorch で行います。`TorchFM` クラスでは、機械学習モデルとしての $g(\\boldsymbol{x})$ を定義します。下式の通り、$g(\\boldsymbol{x})$ 内の各項は、`TorchFM` クラス内の `out_lin`、`out_1`、`out_2`、`out_inter` に直接対応します。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  g(\\boldsymbol{x} | \\boldsymbol{w}, \\boldsymbol{v}) &= \n",
    "  \\underset{\\color{red}{\\mathtt{out\\_lin}}}{\\underline{ w_0 + \\sum_{i=1}^D w_i x_i} } + \\underset{\\color{red}{\\mathtt{out\\_inter}}}{\\underline{\\frac{1}{2}\n",
    "  \\left[\\underset{\\color{red}{\\mathtt{out\\_1}}}{\\underline{ \\sum_{f=1}^k\\left(\\sum_{i=1}^D v_{i f} x_i\\right)^2 }} - \\underset{\\color{red}{\\mathtt{out\\_2}}}{\\underline{ \\sum_{f=1}^k\\sum_{i=1}^D v_{i f}^2 x_i^2 }} \\right] }}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TorchFM(nn.Module):\n",
    "    def __init__(self, d: int, k: int):\n",
    "        super().__init__()\n",
    "        self.V = nn.Parameter(torch.randn(d, k), requires_grad=True)\n",
    "        self.lin = nn.Linear(d, 1)  # 右辺第1項及び2項は全結合ネットワーク\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)\n",
    "        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)\n",
    "        out_inter = 0.5 * (out_1 - out_2)\n",
    "        out_lin = self.lin(x)\n",
    "        out = out_inter + out_lin\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、入出力データから FM を機械学習する関数 `train()` を定義します。一般的な機械学習と同様に、教師データを学習データと検証データに分割し、学習データを用いてパラメータの最適化、検証データを用いて学習中のモデル検証を行います。`train()` 関数は、検証データに対して最も予測精度の高かったモデルを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    model_class=None,\n",
    "    model_params=None,\n",
    "    batch_size=1024,\n",
    "    epochs=3000,\n",
    "    criterion=None,\n",
    "    optimizer_class=None,\n",
    "    opt_params=None,\n",
    "    lr_sche_class=None,\n",
    "    lr_sche_params=None,\n",
    "):\n",
    "    X_tensor, y_tensor = (\n",
    "        torch.from_numpy(X).float(),\n",
    "        torch.from_numpy(y).float(),\n",
    "    )\n",
    "    indices = np.array(range(X.shape[0]))\n",
    "    indices_train, indices_valid = train_test_split(\n",
    "        indices, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = TensorDataset(X_tensor[indices_train], y_tensor[indices_train])\n",
    "    valid_set = TensorDataset(X_tensor[indices_valid], y_tensor[indices_valid])\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        \"valid\": DataLoader(valid_set, batch_size=batch_size, shuffle=False),\n",
    "    }\n",
    "\n",
    "    model = model_class(**model_params)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    optimizer = optimizer_class(model.parameters(), **opt_params)\n",
    "    if lr_sche_class is not None:\n",
    "        scheduler = lr_sche_class(optimizer, **lr_sche_params)\n",
    "    best_score = 1e18\n",
    "    for epoch in range(epochs):\n",
    "        losses = {\"train\": 0.0, \"valid\": 0.0}\n",
    "\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for batch_x, batch_y in loaders[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(batch_x).T[0]\n",
    "                loss = criterion(out, batch_y)\n",
    "                losses[phase] += loss.item() * batch_x.size(0)\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "            losses[phase] /= len(loaders[phase].dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            if best_score > losses[\"valid\"]:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_score = losses[\"valid\"]\n",
    "        if lr_sche_class is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_4\"></a>\n",
    "### 2.4\\. 初期教師データの作成\n",
    "\n",
    "入力値 $\\boldsymbol{x}$ に対して目的関数 $f(\\boldsymbol{x})$ を評価し、$N_0$ 個の入出力ペア（初期教師データ）を作成します。ここでの入力値 $\\boldsymbol{x}$ の決め方は様々ですが、乱数を用いたり、現象に対する知見に基づき機械学習に適した値を用いたりします。過去に実施した実験やシミュレーションの結果から、教師データを構築しても構いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(D: int, N0: int, true_func):\n",
    "    assert N0 < 2**D\n",
    "    # N0個の入力値を乱数を用いて取得\n",
    "    X = np.random.randint(0, 2, size=(N0, D))\n",
    "    # 取得した入力値のうち重複しているものを除外し、除外した分の入力値を乱数を用いて追加\n",
    "    X = np.unique(X, axis=0)\n",
    "    while X.shape[0] != N0:\n",
    "        X = np.vstack((X, np.random.randint(0, 2, size=(N0 - X.shape[0], D))))\n",
    "        X = np.unique(X, axis=0)\n",
    "    y = np.zeros(N0)\n",
    "    # N0個の入力値に対応する出力値を目的関数を評価して取得\n",
    "    for i in range(N0):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Generating {i}-th training data set.\")\n",
    "        y[i] = true_func(X[i])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2_5\"></a>\n",
    "### 2.5\\. FMQA サイクルの実行クラス\n",
    "\n",
    "`FMQA.cycle()` では、事前に準備した初期教師データを用い、FMQA サイクルを $N-N_0$ 回実施します。`FMQA.step()` は、FMQA を1サイクルのみ行う関数で、`FMQA.cycle()` から $N-N_0$ 回呼び出されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amplify import (\n",
    "    Solver,\n",
    "    BinarySymbolGenerator,\n",
    "    sum_poly,\n",
    "    BinaryMatrix,\n",
    "    BinaryQuadraticModel,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "class FMQA:\n",
    "    def __init__(self, D: int, N: int, N0: int, k: int, true_func, solver) -> None:\n",
    "        assert N0 < N\n",
    "        self.D = D\n",
    "        self.N = N\n",
    "        self.N0 = N0\n",
    "        self.k = k\n",
    "        self.true_func = true_func\n",
    "        self.solver = solver\n",
    "        self.y = None\n",
    "\n",
    "    # 教師データに基づいて N-N0 回のFMQAを教師データを追加しながら繰り返し実施するメンバー関数\n",
    "    def cycle(self, X, y, log=False) -> np.ndarray:\n",
    "        print(f\"Starting FMQA cycles...\")\n",
    "        pred_x = X[0]\n",
    "        pred_y = 1e18\n",
    "        for i in range(self.N - self.N0):\n",
    "            print(f\"FMQA Cycle #{i} \", end=\"\")\n",
    "            try:\n",
    "                x_hat = self.step(X, y)\n",
    "            except RuntimeError:\n",
    "                sys.exit(f\"Unknown error, i = {i}\")\n",
    "            # x_hat として既に全く同じ入力が教師データ内に存在する場合、その周辺の値を x_hat とする。\n",
    "            is_identical = True\n",
    "            while is_identical:\n",
    "                is_identical = False\n",
    "                for j in range(i + self.N0):\n",
    "                    if np.all(x_hat == X[j, :]):\n",
    "                        change_id = np.random.randint(0, self.D, 1)\n",
    "                        x_hat[change_id.item()] = 1 - x_hat[change_id.item()]\n",
    "                        if log:\n",
    "                            print(f\"{i=}, Identical x is found, {x_hat=}\")\n",
    "                        is_identical = True\n",
    "                        break\n",
    "            # hat{x} で目的関数 f() を評価\n",
    "            y_hat = self.true_func(x_hat)\n",
    "            # 最適点近傍における入出力ペア [x_hat, y_hat] を教師データに追加\n",
    "            X = np.vstack((X, x_hat))\n",
    "            y = np.append(y, y_hat)\n",
    "            # 目的関数の評価値が最小値を更新したら、その入出力ペアを [pred_x, pred_y] へコピー\n",
    "            if pred_y > y_hat:\n",
    "                pred_y = y_hat\n",
    "                pred_x = x_hat\n",
    "                print(f\"variable updated, {pred_y=}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "            # 全ての入力を全探索済みの場合は、for文を抜ける\n",
    "            if len(y) >= 2**self.D:\n",
    "                print(f\"Fully searched at {i=}. Terminating FMQA cycles.\")\n",
    "                break\n",
    "        self.y = y\n",
    "        return pred_x\n",
    "\n",
    "    # 1回のFMQAを実施するメンバー関数\n",
    "    def step(self, X, y) -> np.ndarray:\n",
    "        # FM を機械学習\n",
    "        model = train(\n",
    "            X,\n",
    "            y,\n",
    "            model_class=TorchFM,\n",
    "            model_params={\"d\": self.D, \"k\": self.k},\n",
    "            batch_size=8,\n",
    "            epochs=2000,\n",
    "            criterion=nn.MSELoss(),\n",
    "            optimizer_class=torch.optim.AdamW,\n",
    "            opt_params={\"lr\": 1},\n",
    "        )\n",
    "        # 学習済みモデルから、FM パラメータの抽出\n",
    "        v, w, w0 = list(model.parameters())\n",
    "        v = v.detach().numpy()\n",
    "        w = w.detach().numpy()[0]\n",
    "        w0 = w0.detach().numpy()[0]\n",
    "        # ここから量子アニーリング・イジングマシンによる求解を実施\n",
    "        gen = BinarySymbolGenerator()  # BinaryPoly の変数ジェネレータを宣言\n",
    "        q = gen.array(self.D)  # BinaryPoly から決定変数の作成\n",
    "        cost = self.__FM_as_QUBO(q, w0, w, v)  # FM パラメータから QUBO として FM を定義\n",
    "        result = self.solver.solve(cost)  # 目的関数を Amplify のソルバーに受け渡し\n",
    "        if len(result.solutions) == 0:\n",
    "            raise RuntimeError(\"No solution was found.\")\n",
    "        values = result.solutions[0].values\n",
    "        q_values = q.decode(values)\n",
    "        return q_values\n",
    "\n",
    "    # FM パラメータから QUBO として FM を定義する関数。前定義の TorchFM クラスと同様に、g(x) の関数形通りに数式を記述。\n",
    "    def __FM_as_QUBO(self, x, w0, w, v):\n",
    "        lin = w0 + (x.T @ w)\n",
    "        D = w.shape[0]\n",
    "        out_1 = sum_poly(self.k, lambda i: sum_poly(D, lambda j: x[j] * v[j, i]) ** 2)\n",
    "        # 次式において、x[j] はバイナリ変数なので、x[j] = x[j]^2 であることに注意。\n",
    "        out_2 = sum_poly(\n",
    "            self.k, lambda i: sum_poly(D, lambda j: x[j] * v[j, i] * v[j, i])\n",
    "        )\n",
    "        return lin + (out_1 - out_2) / 2\n",
    "\n",
    "    \"\"\"上記の __FM_as_QUBO で用いられている sum_poly は、計算速度やメモリの観点から非効率。一般的に決定変数の相互作用項が非ゼロである FM の場合、BinaryMatrix を使う次の書き方が効率的。ここで、BinaryMatrixでの2次項は、上三角行列で表される非対角項に対応するため、FM式の2次の項に対する x(1/2) は不要。また、上の __FM_as_QUBO（sum_poly を使う実装）と関数のシグネチャを合わせるために、x を引数に取っているが、BinaryMatrix を使う本実装では本来は不要。\n",
    "    def __FM_as_QUBO(self, x, w0, w, v):\n",
    "        out_1_matrix = v @ v.T\n",
    "        out_2_matrix = np.diag((v * v).sum(axis=1))\n",
    "        matrix = BinaryMatrix(out_1_matrix - out_2_matrix + np.diag(w))\n",
    "        # 定数項 w0 を忘れずに BinaryQuadraticModel の2つ目の引数に入れる。\n",
    "        model = BinaryQuadraticModel(matrix, w0)\n",
    "        return model\n",
    "    \"\"\"\n",
    "\n",
    "    # 初期教師データ及び各 FMQA サイクル内で実施した i 回の目的関数評価値の履歴をプロットする関数\n",
    "    def plot_history(self):\n",
    "        assert self.y is not None\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        plt.plot(\n",
    "            [i for i in range(self.N0)],\n",
    "            self.y[: self.N0],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"b\",\n",
    "        )  # 初期教師データ生成時の目的関数評価値（ランダム過程）\n",
    "        plt.plot(\n",
    "            [i for i in range(self.N0, self.N)],\n",
    "            self.y[self.N0 :],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            color=\"r\",\n",
    "        )  # FMQA サイクル時の目的関数評価値（FMQA サイクル過程）\n",
    "        plt.xlabel(\"i-th evaluation of f(x)\", fontsize=18)\n",
    "        plt.ylabel(\"f(x)\", fontsize=18)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3\\. FMQA 実行例\n",
    "\n",
    "<a id=\"3_1\"></a>\n",
    "### 3.1\\. $\\boldsymbol{x}$の2次式に対する最適化\n",
    "\n",
    "本サンプルコードで最適化対象とする代数式は、次の2次式です。$Q$ は乱数で生成される成分の平均値がゼロな $d$ 次元対象行列であり、`make_Q` で作成されます。\n",
    "\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x}) = \\boldsymbol{x}^T Q \\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "本来 FMQA は、未知関数に対して実施されるものであることに注意してください。今回は、簡単な理解のため、既知関数である上記 $f(\\boldsymbol{x})$ を、未知関数（＝ブラックボックス）として取り扱っています。\n",
    "\n",
    "以下の条件（$D=100$、$N=100$、$N_0=70$）では、FMQA のサイクル完了までにおよそ数分の計算時間を要しますので、ご注意下さい。出力例を、『[3.3\\. FMQA サンプルコード実行例](#3_3)』に紹介しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d次元対称行列であって成分の平均が0のものを出力\n",
    "def make_Q(d) -> np.ndarray:\n",
    "    Q_true = np.random.rand(d, d)\n",
    "    Q_true = (Q_true + Q_true.T) / 2\n",
    "    Q_true = Q_true - np.mean(Q_true)\n",
    "    return Q_true\n",
    "\n",
    "\n",
    "# 乱数シード値を初期化\n",
    "seed_everything(0)\n",
    "\n",
    "# 入力値の次元（問題サイズ）\n",
    "D = 100\n",
    "# 真の関数で使われる行列 Q\n",
    "Q = make_Q(D)\n",
    "# 目的関数（xQx）の定義。\n",
    "\n",
    "\n",
    "def true_func(x):\n",
    "    # 本来は、未知関数（シミュレーションや実験）の結果値やその後処理結果値を cost とする。\n",
    "    cost = x @ Q @ x\n",
    "    return cost\n",
    "\n",
    "\n",
    "N = 70  # 関数を評価できる回数\n",
    "N0 = 60  # 初期教師データのサンプル数\n",
    "k = 10  # FMにおけるベクトルの次元（ハイパーパラメータ）\n",
    "\n",
    "# client：先に作成した Amplify クライアント\n",
    "solver = Solver(client)\n",
    "# 初期教師データの生成\n",
    "X, y = gen_training_data(D, N0, true_func)\n",
    "\n",
    "# FMQA のインスタンス化\n",
    "fmqa_solver = FMQA(D, N, N0, k, true_func, solver)\n",
    "# FMQA サイクルの実行\n",
    "pred_x = fmqa_solver.cycle(X, y)\n",
    "# 最適化結果の出力\n",
    "print(\"pred x:\", pred_x)\n",
    "print(\"pred value:\", true_func(pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2\"></a>\n",
    "### 3.2\\. FMQA 最適化過程における目的関数値の推移\n",
    "\n",
    "初期教師データ作成時にランダムに生成した入力値に対して得られた $N_0$ 個の目標関数値及び $N-N_0$ サイクルの FMQA 最適化過程における目標関数値の推移を以下にプロットします。それぞれ、青色及び赤色で示されています。\n",
    "\n",
    "FMQA 最適化サイクルにより得られた入力値 $\\hat{x}$ により、目的関数値の最小値が次々と更新される様子が示されています（出力例を、『[3.3\\. FMQA サンプルコード実行例](#3_3)』に紹介しています）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = fmqa_solver.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_3\"></a>\n",
    "### 3.3\\. FMQA サンプルコード実行例\n",
    "\n",
    "一般的に、`FixstarsClient` で採用されているヒューリスティクスというアルゴリズムの原理上、得られる解に完全な再現性はありませんが、本サンプルコードを実行した際に得られる、典型的な標準出力及び画像出力を以下に紹介します。※得られる値が多少異なる場合がございます。\n",
    "\n",
    "- 『[3.1. $\\boldsymbol{x}$の2次式に対する最適化](#3_1)』に記載の FMQA コードを与えられた条件のまま実行すると、次のような標準出力が逐次出力されます。\n",
    "\n",
    "    ```shell\n",
    "    Generating 0-th training data set.\n",
    "    Generating 10-th training data set.\n",
    "    Generating 20-th training data set.\n",
    "    Generating 30-th training data set.\n",
    "    Generating 40-th training data set.\n",
    "    Generating 50-th training data set.\n",
    "    Starting FMQA cycles...\n",
    "    FMQA Cycle #0 variable updated, pred_y=-59.15752919611154\n",
    "    FMQA Cycle #1 \n",
    "    FMQA Cycle #2 variable updated, pred_y=-72.66802296872575\n",
    "    FMQA Cycle #3 \n",
    "    FMQA Cycle #4 \n",
    "    FMQA Cycle #5 \n",
    "    FMQA Cycle #6 \n",
    "    FMQA Cycle #7 \n",
    "    FMQA Cycle #8 variable updated, pred_y=-76.81540215271143\n",
    "    FMQA Cycle #9 \n",
    "    pred x: [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
    "    1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
    "    2. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
    "    3. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
    "    4. 0. 0. 1.]\n",
    "    pred value: -76.81540215271143\n",
    "    ```\n",
    "\n",
    "- 『[3.2．FMQA 最適化過程における目的関数値の推移](#3_2)』に記載の `fmqa_solver.plot_history()` の出力画像は次のようになります。\n",
    "\n",
    "    ![plot_history](../figures/fmqa_0_algebra_plot_history.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_4\"></a>\n",
    "### 3.4\\. まとめ\n",
    "\n",
    "\n",
    "今回は、比較的単純な既知関数を対象として、FMQA による最適化を実施しました。しかし、本来、FMQA が力を発揮するのは、強い非線形性を有する物理現象や社会現象など、評価にシミュレーションや実験計測が必要な未知関数に対する最適化です。Amplify では、そのようなより現実的なモデルケースにおける FMQA の実施例やサンプルコードも紹介しています。\n",
    "\n",
    "- [ブラックボックス最適化によるモデル超電導材料の探索](./fmqa_1_supercon.ipynb)\n",
    "- [ブラックボックス最適化による化学プラントにおける生産量最大化](./fmqa_2_reactor.ipynb)\n",
    "- [ブラックボックス最適化と流体シミュレーションによる翼形状の最適化](./fmqa_3_aerofoil.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_5\"></a>\n",
    "### 3.5\\. 参考\n",
    "\n",
    "今回、最適化の対象であった $f(x) = x^{\\top}Qx$ は2次式であり、数式が既知であるため、FMQA を使わずに直接組み合わせ最適化による最適入力値の探索が可能です。以下のコードでは、本関数に対して直接、量子アニーリング・イジングマシンによる最適化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryPoly の変数ジェネレータを宣言\n",
    "gen = BinarySymbolGenerator()\n",
    "# サイズ D を有する決定変数の1次元配列を作成\n",
    "q = gen.array(D)\n",
    "# xQx を QUBO の目的関数として定式化\n",
    "cost = sum_poly(D, lambda i: sum_poly(D, lambda j: Q[i, j] * q[i] * q[j]))\n",
    "# 目的関数を Amplify に渡し、求解を行う。\n",
    "result = solver.solve(cost)\n",
    "if len(result.solutions) == 0:\n",
    "    raise RuntimeError(\"No solution was found.\")\n",
    "# 推定された最適解を抽出し表示する。\n",
    "values = result.solutions[0].values\n",
    "true_x = q.decode(values)\n",
    "print(\"true x:\", true_x)\n",
    "print(\"true value:\", true_func(true_x))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 }
}
